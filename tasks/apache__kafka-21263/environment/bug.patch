diff --git a/core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala b/core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala
index 35c44b9524..afc982aae0 100644
--- a/core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala
+++ b/core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala
@@ -23,6 +23,7 @@ import kafka.log.LogManager
 import kafka.server.share.SharePartitionManager
 import kafka.server.{KafkaConfig, ReplicaManager}
 import kafka.utils.Logging
+import org.apache.kafka.common.TopicPartition
 import org.apache.kafka.common.errors.TimeoutException
 import org.apache.kafka.common.internals.Topic
 import org.apache.kafka.coordinator.group.GroupCoordinator
@@ -39,6 +40,7 @@ import org.apache.kafka.server.fault.FaultHandler
 import org.apache.kafka.storage.internals.log.{LogManager => JLogManager}
 
 import java.util.concurrent.CompletableFuture
+import scala.collection.mutable
 import scala.jdk.CollectionConverters._
 
 
@@ -184,6 +186,22 @@ class BrokerMetadataPublisher(
           case t: Throwable => metadataPublishingFaultHandler.handleFault("Error updating share " +
             s"coordinator with local changes in $deltaName", t)
         }
+        try {
+          // Notify the group coordinator about deleted topics.
+          val deletedTopicPartitions = new mutable.ArrayBuffer[TopicPartition]()
+          topicsDelta.deletedTopicIds().forEach { id =>
+            val topicImage = topicsDelta.image().getTopic(id)
+            topicImage.partitions().keySet().forEach {
+              id => deletedTopicPartitions += new TopicPartition(topicImage.name(), id)
+            }
+          }
+          if (deletedTopicPartitions.nonEmpty) {
+            groupCoordinator.onPartitionsDeleted(deletedTopicPartitions.asJava, RequestLocal.noCaching.bufferSupplier)
+          }
+        } catch {
+          case t: Throwable => metadataPublishingFaultHandler.handleFault("Error updating group " +
+            s"coordinator with deleted partitions in $deltaName", t)
+        }
         try {
           // Notify the share coordinator about deleted topics.
           val deletedTopicIds = topicsDelta.deletedTopicIds()
diff --git a/core/src/test/scala/unit/kafka/server/OffsetFetchRequestTest.scala b/core/src/test/scala/unit/kafka/server/OffsetFetchRequestTest.scala
index ff228f245c..f838bb42d9 100644
--- a/core/src/test/scala/unit/kafka/server/OffsetFetchRequestTest.scala
+++ b/core/src/test/scala/unit/kafka/server/OffsetFetchRequestTest.scala
@@ -568,7 +568,7 @@ class OffsetFetchRequestTest(cluster: ClusterInstance) extends GroupCoordinatorB
   @ClusterTest
   def testFetchOffsetWithRecreatedTopic(): Unit = {
     // There are two ways to ensure that committed of recreated topics are not returned.
-    // 1) When a topic is deleted, GroupCoordinatorService#onMetadataUpdate is called to
+    // 1) When a topic is deleted, GroupCoordinatorService#onPartitionsDeleted is called to
     //    delete all its committed offsets.
     // 2) Since version 10 of the OffsetCommit API, the topic id is stored alongside the
     //    committed offset. When it is queried, it is only returned iff the topic id of
diff --git a/docs/streams/developer-guide/app-reset-tool.md b/docs/streams/developer-guide/app-reset-tool.md
index ea0dbede1b..bc69cff10c 100644
--- a/docs/streams/developer-guide/app-reset-tool.md
+++ b/docs/streams/developer-guide/app-reset-tool.md
@@ -56,7 +56,7 @@ Prerequisites
 
 # Step 1: Run the application reset tool
 
-If you are using **streams rebalance protocol** (available since AK 4.2), use the [Streams groups CLI](kafka-streams-group-sh.html#reset-offsets).
+If you are using **streams rebalance protocol** (available since AK 4.2), use the [Streams groups CLI](kafka-streams-group-sh#reset-offsets).
 
 If you are using **classic rebalance protocol** , run the classic application reset tool as described below.
 
diff --git a/docs/streams/developer-guide/kafka-streams-group-sh.md b/docs/streams/developer-guide/kafka-streams-group-sh.md
new file mode 100644
index 0000000000..d38a884f49
--- /dev/null
+++ b/docs/streams/developer-guide/kafka-streams-group-sh.md
@@ -0,0 +1,183 @@
+---
+title: Kafka Streams Groups Tool
+type: docs
+description: 
+weight: 14
+tags: ['kafka', 'docs']
+aliases: 
+keywords: 
+---
+
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+Use `kafka-streams-groups.sh` to manage **Streams groups** for the Streams Rebalance Protocol (KIP‑1071): list and describe groups, inspect members and offsets/lag, reset or delete offsets for input topics, and delete groups (optionally including internal topics).
+
+
+# Overview
+
+A **Streams group** is a broker‑coordinated group type for Kafka Streams that uses Streams‑specific RPCs and metadata, distinct from classic consumer groups. The CLI surfaces Streams‑specific states, assignments, and input‑topic offsets to simplify visibility and administration.
+
+**Use with care:** Mutating operations (offset resets/deletes, group deletion) affect how applications will reprocess data when restarted. Always preview with \--dry-run before executing and ensure application instances are stopped/inactive and the group is empty before executing the command. 
+
+# What the Streams Groups tool does
+
+  * **List Streams groups** across a cluster and display or filter by group state (Empty, Not Ready, Assigning, Reconciling, Stable, Dead).
+  * **Describe a Streams group** and show: 
+    * Group state, group epoch, target assignment epoch (with `--state`, `--verbose` for additional details).
+    * Per‑member info such as epochs, current vs target assignments, and whether a member still uses the classic protocol (with `--members` and `--verbose`).
+    * Input‑topic offsets and lag (with `--offsets`), to understand how far behind processing is.
+  * **Reset input‑topic offsets** for a Streams group to control reprocessing boundaries using precise specifiers (earliest, latest, to‑offset, to‑datetime, by‑duration, shift‑by, from‑file). Requires `--dry-run` or `--execute` and inactive instances.
+  * **Delete offsets** for input topics to force re‑consumption on next start.
+  * **Delete a Streams group** to clean up broker‑side Streams metadata (offsets, topology, assignments). Optionally delete all, or a subset of, **internal topics** at the same time using `--internal-topics`.
+
+
+
+# Usage
+
+The script is located in `bin/kafka-streams-groups.sh` and connects to your cluster via `--bootstrap-server`. For secured clusters, pass AdminClient properties using `--command-config`.
+    
+    
+    $ kafka-streams-groups.sh --bootstrap-server <host:port> [COMMAND] [OPTIONS]
+
+**Note:** `kafka-streams-groups.sh` complements the Streams Admin API for Streams groups. The CLI exposes list/describe/delete operations and offset management similar in spirit to consumer-group tools, but tailored to Streams groups defined in KIP‑1071. 
+
+# Commands
+
+## List Streams groups
+
+Discovering groups
+    
+    
+    # List all Streams groups
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 --list
+    
+
+## Describe Streams groups
+
+Inspecting group's state, members, and lag
+    
+    
+    # Describe a group: state + epochs
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --describe --group my-streams-app --state --verbose
+    
+    # Describe a group: members (assignments vs target, classic/streams)
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --describe --group my-streams-app --members --verbose
+    
+    # Describe a group: input-topic offsets and lag
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --describe --group my-streams-app --offsets
+    
+
+## Reset input-topic offsets (preview, then apply) {#reset-offsets}
+
+Ensure all application instances are stopped/inactive. Always preview changes with `--dry-run` before using `--execute`.
+    
+    
+    # Preview resetting all input topics to a specific timestamp
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --group my-streams-app \
+      --reset-offsets --all-input-topics --to-datetime 2025-01-31T23:57:00.000 \
+      --dry-run
+    
+    # Apply the reset
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --group my-streams-app \
+      --reset-offsets --all-input-topics --to-datetime 2025-01-31T23:57:00.000 \
+      --execute
+    
+
+## Delete offsets to force re-consumption
+
+Delete offsets for all or specific input topics to have the group re-read data on restart.
+    
+    
+    # Delete offsets for all input topics (execute)
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --group my-streams-app \
+      --delete-offsets --all-input-topics --execute
+    
+    # Delete offsets for specific topics
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --group my-streams-app \
+      --delete-offsets --topic input-a --topic input-b --execute
+    
+
+## Delete a Streams group (cleanup)
+
+Delete broker-side Streams metadata for a group and optionally remove a subset of internal topics.
+    
+    
+    # Delete Streams group metadata
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --delete --group my-streams-app
+    
+    # Delete a subset of internal topics alongside the group (use with care)
+    kafka-streams-groups.sh --bootstrap-server localhost:9092 \
+      --delete --group my-streams-app \
+      --internal-topics my-app-repartition-0,my-app-changelog
+    
+
+# All options and flags
+
+## Core actions
+
+  * `--list`: List Streams groups. Use `--state` to display/filter by state.
+  * `--describe`: Describe a group selected by `--group`. Combine with: 
+    * `--state` (group state and epochs), `--members` (members and assignments), `--offsets` (input and repartition topics offsets/lag).
+    * `--verbose` for additional details (e.g., leader epochs where applicable).
+  * `--reset-offsets`: Reset input-topic offsets (one group at a time; instances should be inactive). Choose exactly one specifier: 
+    * `--to-earliest`, `--to-latest`, `--to-current`, `--to-offset <n>`
+    * `--by-duration <PnDTnHnMnS>`, `--to-datetime <YYYY-MM-DDTHH:mm:SS.sss>`
+    * `--shift-by <n>` (±), `--from-file` (CSV)
+Scope: 
+    * `--all-input-topics` or one/more `--topic <name>`; some builds also support `--all-topics` (all input topics per broker topology metadata).
+Safety: 
+    * Requires `--dry-run` or `--execute`.
+  * `--delete-offsets`: Delete offsets for `--all-input-topics`, specific `--topic` names, or `--from-file`.
+  * `--delete`: Delete Streams group metadata; optionally pass `--internal-topics <list>` to delete a subset of internal topics.
+
+
+
+## Common flags
+
+  * `--group <id>`: Target Streams group (application.id).
+  * `--all-groups`: Operate on all groups (allowed with `--delete`).
+  * `--bootstrap-server <host:port>`: Broker(s) to connect to (required).
+  * `--command-config <file>`: Properties for AdminClient (security, timeouts, etc.).
+  * `--timeout <ms>`: Wait time for group stabilization in some operations (default: 5000ms).
+  * `--dry-run`, `--execute`: Preview vs apply for mutating operations.
+  * `--help`, `--version`, `--verbose`: Usage, version, verbosity.
+
+
+
+# Best practices and safety
+
+  * Preview changes with `--dry-run` to verify topic scope and impact before `--execute`.
+  * Use `--internal-topics` carefully: deleting internal topics removes state backing topics; only do this when you intend to rebuild state from input topics.
+
+
+
+This page documents `kafka-streams-groups.sh` capabilities for Streams groups as defined by KIP‑1071 and implemented in Apache Kafka.
+
+  * [Documentation](/documentation)
+  * [Kafka Streams](/documentation/streams)
+  * [Developer Guide](/documentation/streams/developer-guide/)
+
+
diff --git a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java
index 9eb6d1cfd3..753d1736f7 100644
--- a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java
+++ b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java
@@ -64,6 +64,7 @@ import java.util.Optional;
 import java.util.OptionalInt;
 import java.util.Properties;
 import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ExecutionException;
 import java.util.function.IntSupplier;
 
 /**
@@ -409,6 +410,17 @@ public interface GroupCoordinator {
      */
     int partitionFor(String groupId);
 
+    /**
+     * Remove the provided deleted partitions offsets.
+     *
+     * @param topicPartitions   The deleted partitions.
+     * @param bufferSupplier    The buffer supplier tight to the request thread.
+     */
+    void onPartitionsDeleted(
+        List<TopicPartition> topicPartitions,
+        BufferSupplier bufferSupplier
+    ) throws ExecutionException, InterruptedException;
+
     /**
      * Group coordinator is now the leader for the given partition at the
      * given leader epoch. It should load cached state from the partition
diff --git a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java
index f8dc5068db..701e58f96c 100644
--- a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java
+++ b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java
@@ -100,7 +100,6 @@ import org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetrics;
 import org.apache.kafka.coordinator.group.streams.StreamsGroupHeartbeatResult;
 import org.apache.kafka.image.MetadataDelta;
 import org.apache.kafka.image.MetadataImage;
-import org.apache.kafka.image.TopicsDelta;
 import org.apache.kafka.server.authorizer.AuthorizableRequestContext;
 import org.apache.kafka.server.authorizer.Authorizer;
 import org.apache.kafka.server.record.BrokerCompressionType;
@@ -137,6 +136,7 @@ import java.util.Properties;
 import java.util.Set;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.CompletionException;
+import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executors;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.IntSupplier;
@@ -2217,6 +2217,63 @@ public class GroupCoordinatorService implements GroupCoordinator {
         );
     }
 
+    /**
+     * See {@link GroupCoordinator#onPartitionsDeleted(List, BufferSupplier)}.
+     */
+    @Override
+    public void onPartitionsDeleted(
+        List<TopicPartition> topicPartitions,
+        BufferSupplier bufferSupplier
+    ) throws ExecutionException, InterruptedException {
+        throwIfNotActive();
+
+        var futures = new ArrayList<CompletableFuture<Void>>();
+
+        // Handle the partition deletion for committed offsets.
+        futures.addAll(
+            FutureUtils.mapExceptionally(
+                runtime.scheduleWriteAllOperation(
+                    "on-partition-deleted",
+                    Duration.ofMillis(config.offsetCommitTimeoutMs()),
+                    coordinator -> coordinator.onPartitionsDeleted(topicPartitions)
+                ),
+                exception -> {
+                    log.error("Could not delete offsets for deleted partitions {} due to: {}.",
+                        topicPartitions, exception.getMessage(), exception
+                    );
+                    return null;
+                }
+            )
+        );
+
+        // Handle the topic deletion for share state.
+        if (metadataImage != null) {
+            var topicIds = topicPartitions.stream()
+                .filter(tp -> metadataImage.topicMetadata(tp.topic()).isPresent())
+                .map(tp -> metadataImage.topicMetadata(tp.topic()).get().id())
+                .collect(Collectors.toSet());
+
+            if (!topicIds.isEmpty()) {
+                futures.addAll(
+                    FutureUtils.mapExceptionally(
+                        runtime.scheduleWriteAllOperation(
+                            "maybe-cleanup-share-group-state",
+                            Duration.ofMillis(config.offsetCommitTimeoutMs()),
+                            coordinator -> coordinator.maybeCleanupShareGroupState(topicIds)
+                        ),
+                        exception -> {
+                            log.error("Unable to cleanup state for the deleted topics {}", topicIds, exception);
+                            return null;
+                        }
+                    )
+                );
+            }
+        }
+
+        // Wait on the results.
+        CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[0]));
+    }
+
     /**
      * See {@link GroupCoordinator#onElection(int, int)}.
      */
@@ -2258,77 +2315,10 @@ public class GroupCoordinatorService implements GroupCoordinator {
         throwIfNotActive();
         Objects.requireNonNull(delta, "delta must be provided");
         Objects.requireNonNull(newImage, "newImage must be provided");
-
-        // Update the metadata image and propagate to runtime.
         var wrappedImage = new KRaftCoordinatorMetadataImage(newImage);
         var wrappedDelta = new KRaftCoordinatorMetadataDelta(delta);
         metadataImage = wrappedImage;
         runtime.onMetadataUpdate(wrappedDelta, wrappedImage);
-
-        // Handle partition deletions from the delta.
-        if (delta.topicsDelta() != null && !delta.topicsDelta().deletedTopicIds().isEmpty()) {
-            handlePartitionsDeletion(delta.topicsDelta());
-        }
-    }
-
-    /**
-     * Handles the deletion of topic partitions by scheduling write operations
-     * to delete committed offsets and clean up share group state.
-     *
-     * @param topicsDelta The topics delta containing deleted topic IDs.
-     */
-    private void handlePartitionsDeletion(TopicsDelta topicsDelta) {
-        var topicPartitions = new ArrayList<TopicPartition>();
-        var topicIds = topicsDelta.deletedTopicIds();
-
-        topicIds.forEach(topicId -> {
-            var topicImage = topicsDelta.image().getTopic(topicId);
-            if (topicImage != null) {
-                topicImage.partitions().keySet().forEach(partitionId ->
-                    topicPartitions.add(new TopicPartition(topicImage.name(), partitionId))
-                );
-            }
-        });
-
-        var futures = new ArrayList<CompletableFuture<Void>>();
-
-        if (!topicPartitions.isEmpty()) {
-            // Schedule offset deletion.
-            futures.addAll(
-                FutureUtils.mapExceptionally(
-                    runtime.scheduleWriteAllOperation(
-                        "on-partition-deleted",
-                        Duration.ofMillis(config.offsetCommitTimeoutMs()),
-                        coordinator -> coordinator.onPartitionsDeleted(topicPartitions)
-                    ),
-                    exception -> {
-                        log.error("Could not delete offsets for deleted partitions {} due to: {}.",
-                            topicPartitions, exception.getMessage(), exception);
-                        return null;
-                    }
-                )
-            );
-        }
-
-        if (!topicIds.isEmpty()) {
-            // Schedule share group state cleanup.
-            futures.addAll(
-                FutureUtils.mapExceptionally(
-                    runtime.scheduleWriteAllOperation(
-                        "maybe-cleanup-share-group-state",
-                        Duration.ofMillis(config.offsetCommitTimeoutMs()),
-                        coordinator -> coordinator.maybeCleanupShareGroupState(topicIds)
-                    ),
-                    exception -> {
-                        log.error("Unable to cleanup state for the deleted topics {}", topicIds, exception);
-                        return null;
-                    }
-                )
-            );
-        }
-
-        // Wait for all operations to complete.
-        CompletableFuture.allOf(futures.toArray(new CompletableFuture<?>[0])).join();
     }
 
     /**
diff --git a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java
index 30fdc37e2b..df91166629 100644
--- a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java
+++ b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java
@@ -76,7 +76,6 @@ import org.apache.kafka.common.message.SyncGroupRequestData;
 import org.apache.kafka.common.message.SyncGroupResponseData;
 import org.apache.kafka.common.message.TxnOffsetCommitRequestData;
 import org.apache.kafka.common.message.TxnOffsetCommitResponseData;
-import org.apache.kafka.common.metadata.RemoveTopicRecord;
 import org.apache.kafka.common.network.ClientInformation;
 import org.apache.kafka.common.network.ListenerName;
 import org.apache.kafka.common.protocol.ApiKeys;
@@ -96,7 +95,6 @@ import org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetrics;
 import org.apache.kafka.coordinator.group.streams.StreamsGroupHeartbeatResult;
 import org.apache.kafka.image.MetadataDelta;
 import org.apache.kafka.image.MetadataImage;
-import org.apache.kafka.image.MetadataProvenance;
 import org.apache.kafka.server.authorizer.AuthorizableRequestContext;
 import org.apache.kafka.server.common.TransactionVersion;
 import org.apache.kafka.server.record.BrokerCompressionType;
@@ -157,7 +155,6 @@ import static org.junit.jupiter.api.Assertions.assertNull;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.timeout;
 import static org.mockito.Mockito.times;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
@@ -3154,112 +3151,157 @@ public class GroupCoordinatorServiceTest {
     }
 
     @Test
-    public void testOnMetadataUpdateWhenNotStarted() {
-        var runtime = mockRuntime();
-        var service = new GroupCoordinatorServiceBuilder()
+    public void testOnPartitionsDeleted() {
+        CoordinatorRuntime<GroupCoordinatorShard, CoordinatorRecord> runtime = mockRuntime();
+        GroupCoordinatorService service = new GroupCoordinatorServiceBuilder()
             .setConfig(createConfig())
             .setRuntime(runtime)
             .build();
+        service.startup(() -> 3);
 
-        var image = new MetadataImageBuilder()
+        MetadataImage image = new MetadataImageBuilder()
             .addTopic(Uuid.randomUuid(), "foo", 1)
             .build();
-        var delta = new MetadataDelta(image);
 
-        assertThrows(CoordinatorNotAvailableException.class,
-            () -> service.onMetadataUpdate(delta, image));
+        service.onMetadataUpdate(new MetadataDelta(image), image);
+
+        when(runtime.scheduleWriteAllOperation(
+            ArgumentMatchers.eq("on-partition-deleted"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
+            ArgumentMatchers.any()
+        )).thenReturn(Arrays.asList(
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null),
+            FutureUtils.failedFuture(Errors.COORDINATOR_LOAD_IN_PROGRESS.exception())
+        ));
+
+        when(runtime.scheduleWriteAllOperation(
+            ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
+            ArgumentMatchers.any()
+        )).thenReturn(Arrays.asList(
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null)
+        ));
+
+        // The exception is logged and swallowed.
+        assertDoesNotThrow(() ->
+            service.onPartitionsDeleted(
+                List.of(new TopicPartition("foo", 0)),
+                BufferSupplier.NO_CACHING
+            )
+        );
     }
 
     @Test
-    public void testOnMetadataUpdateSchedulesOperationsWhenTopicsDeleted() throws ExecutionException, InterruptedException, TimeoutException {
-        var runtime = mockRuntime();
-        var service = new GroupCoordinatorServiceBuilder()
+    public void testOnPartitionsDeletedWhenServiceIsNotStarted() {
+        CoordinatorRuntime<GroupCoordinatorShard, CoordinatorRecord> runtime = mockRuntime();
+        GroupCoordinatorService service = new GroupCoordinatorServiceBuilder()
             .setConfig(createConfig())
             .setRuntime(runtime)
             .build();
-        service.startup(() -> 3);
 
-        var topicId = Uuid.randomUuid();
-        var initialImage = new MetadataImageBuilder()
-            .addTopic(topicId, "foo", 1)
+        assertThrows(CoordinatorNotAvailableException.class, () -> service.onPartitionsDeleted(
+            List.of(new TopicPartition("foo", 0)),
+            BufferSupplier.NO_CACHING
+        ));
+    }
+
+    @Test
+    public void testOnPartitionsDeletedCleanupShareGroupState() {
+        CoordinatorRuntime<GroupCoordinatorShard, CoordinatorRecord> runtime = mockRuntime();
+        GroupCoordinatorService service = new GroupCoordinatorServiceBuilder()
+            .setConfig(createConfig())
+            .setRuntime(runtime)
             .build();
+        service.startup(() -> 3);
 
-        // Create a delta that deletes the topic.
-        var delta = new MetadataDelta(initialImage);
-        delta.replay(new RemoveTopicRecord().setTopicId(topicId));
-        var newImage = delta.apply(new MetadataProvenance(1, 0, 0L, true));
+        MetadataImage image = new MetadataImageBuilder()
+            .addTopic(Uuid.randomUuid(), "foo", 1)
+            .build();
 
-        // Use incomplete futures to verify method blocks.
-        var offsetFutures = List.of(
-            new CompletableFuture<>(),
-            new CompletableFuture<>(),
-            new CompletableFuture<>()
-        );
-        var shareFutures = List.of(
-            new CompletableFuture<>(),
-            new CompletableFuture<>(),
-            new CompletableFuture<>()
-        );
+        service.onMetadataUpdate(new MetadataDelta(image), image);
 
+        // No error in partition deleted callback
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-partition-deleted"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
-        )).thenReturn(offsetFutures);
+        )).thenReturn(List.of(
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null)
+        ));
 
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
-        )).thenReturn(shareFutures);
-
-        // Run onMetadataUpdate in a separate thread.
-        var resultFuture = CompletableFuture.runAsync(() -> service.onMetadataUpdate(delta, newImage));
+        )).thenReturn(List.of(
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.failedFuture(Errors.COORDINATOR_LOAD_IN_PROGRESS.exception())
+        ));
 
-        // Wait for the operations to be scheduled and verify method is blocked.
-        verify(runtime, timeout(5000).times(1)).scheduleWriteAllOperation(
-            ArgumentMatchers.eq("on-partition-deleted"),
-            ArgumentMatchers.eq(Duration.ofMillis(5000)),
-            ArgumentMatchers.any()
+        // The exception is logged and swallowed.
+        assertDoesNotThrow(() ->
+            service.onPartitionsDeleted(
+                List.of(new TopicPartition("foo", 0)),
+                BufferSupplier.NO_CACHING
+            )
         );
-        verify(runtime, timeout(5000).times(1)).scheduleWriteAllOperation(
+
+        verify(runtime, times(1)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         );
-        assertFalse(resultFuture.isDone());
-
-        // Complete all futures.
-        offsetFutures.forEach(f -> f.complete(null));
-        shareFutures.forEach(f -> f.complete(null));
-
-        // Verify method completes.
-        resultFuture.get(5, TimeUnit.SECONDS);
     }
 
     @Test
-    public void testOnMetadataUpdateDoesNotScheduleOperationsWhenNoTopicsDeleted() {
-        var runtime = mockRuntime();
-        var service = new GroupCoordinatorServiceBuilder()
+    public void testOnPartitionsDeletedCleanupShareGroupStateEmptyMetadata() {
+        CoordinatorRuntime<GroupCoordinatorShard, CoordinatorRecord> runtime = mockRuntime();
+        GroupCoordinatorService service = new GroupCoordinatorServiceBuilder()
             .setConfig(createConfig())
             .setRuntime(runtime)
             .build();
         service.startup(() -> 3);
 
-        // Create an image with a topic and a delta with no deletions.
-        var image = new MetadataImageBuilder()
-            .addTopic(Uuid.randomUuid(), "foo", 1)
+        MetadataImage image = new MetadataImageBuilder()
+            .addTopic(Uuid.randomUuid(), "bar", 1)
             .build();
-        var delta = new MetadataDelta(image);
-
-        assertDoesNotThrow(() -> service.onMetadataUpdate(delta, image));
+        service.onMetadataUpdate(new MetadataDelta(image), image);
 
-        // Verify no operations scheduled.
-        verify(runtime, times(0)).scheduleWriteAllOperation(
+        // No error in partition deleted callback
+        when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-partition-deleted"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
+        )).thenReturn(List.of(
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null)
+        ));
+
+        when(runtime.scheduleWriteAllOperation(
+            ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
+            ArgumentMatchers.any()
+        )).thenReturn(List.of(
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null),
+            CompletableFuture.completedFuture(null)
+        ));
+
+        // The exception is logged and swallowed.
+        assertDoesNotThrow(() ->
+            service.onPartitionsDeleted(
+                List.of(new TopicPartition("foo", 0)),
+                BufferSupplier.NO_CACHING
+            )
         );
+
         verify(runtime, times(0)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
@@ -3268,55 +3310,47 @@ public class GroupCoordinatorServiceTest {
     }
 
     @Test
-    public void testOnMetadataUpdateSwallowsErrorsWhenTopicsDeleted() {
-        var runtime = mockRuntime();
-        var service = new GroupCoordinatorServiceBuilder()
+    public void testOnPartitionsDeletedCleanupShareGroupStateTopicsNotInMetadata() {
+        CoordinatorRuntime<GroupCoordinatorShard, CoordinatorRecord> runtime = mockRuntime();
+        GroupCoordinatorService service = new GroupCoordinatorServiceBuilder()
             .setConfig(createConfig())
             .setRuntime(runtime)
             .build();
         service.startup(() -> 3);
 
-        var topicId = Uuid.randomUuid();
-        var initialImage = new MetadataImageBuilder()
-            .addTopic(topicId, "foo", 1)
-            .build();
-
-        // Create a delta that deletes the topic.
-        var delta = new MetadataDelta(initialImage);
-        delta.replay(new RemoveTopicRecord().setTopicId(topicId));
-        var newImage = delta.apply(new MetadataProvenance(1, 0, 0L, true));
+        MetadataImage image = MetadataImage.EMPTY;
+        service.onMetadataUpdate(new MetadataDelta(image), image);
 
-        // Mock operations with 3 futures, some failing.
+        // No error in partition deleted callback
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-partition-deleted"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
-        )).thenReturn(Arrays.asList(
+        )).thenReturn(List.of(
             CompletableFuture.completedFuture(null),
             CompletableFuture.completedFuture(null),
-            FutureUtils.failedFuture(Errors.COORDINATOR_LOAD_IN_PROGRESS.exception())
+            CompletableFuture.completedFuture(null)
         ));
 
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
-        )).thenReturn(Arrays.asList(
+        )).thenReturn(List.of(
             CompletableFuture.completedFuture(null),
             CompletableFuture.completedFuture(null),
-            FutureUtils.failedFuture(Errors.COORDINATOR_LOAD_IN_PROGRESS.exception())
+            CompletableFuture.completedFuture(null)
         ));
 
-        // Verify no exception thrown.
-        assertDoesNotThrow(() -> service.onMetadataUpdate(delta, newImage));
-
-        // Verify operations were still scheduled exactly once.
-        verify(runtime, times(1)).scheduleWriteAllOperation(
-            ArgumentMatchers.eq("on-partition-deleted"),
-            ArgumentMatchers.eq(Duration.ofMillis(5000)),
-            ArgumentMatchers.any()
+        // The exception is logged and swallowed.
+        assertDoesNotThrow(() ->
+            service.onPartitionsDeleted(
+                List.of(new TopicPartition("foo", 0)),
+                BufferSupplier.NO_CACHING
+            )
         );
-        verify(runtime, times(1)).scheduleWriteAllOperation(
+
+        verify(runtime, times(0)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
             ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
@@ -5999,7 +6033,7 @@ public class GroupCoordinatorServiceTest {
                     .build();
             }
 
-            var service = new GroupCoordinatorService(
+            GroupCoordinatorService service = new GroupCoordinatorService(
                 logContext,
                 config,
                 runtime,
