diff --git a/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImpl.java b/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImpl.java
index 9e6ab57174..28d1c4a039 100644
--- a/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImpl.java
+++ b/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImpl.java
@@ -23,6 +23,7 @@ import org.apache.kafka.common.utils.LogContext;
 
 import org.slf4j.Logger;
 
+import java.time.Duration;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ExecutorService;
@@ -35,18 +36,21 @@ public class CoordinatorExecutorImpl<S extends CoordinatorShard<U>, U> implement
     private final TopicPartition shard;
     private final CoordinatorRuntime<S, U> runtime;
     private final ExecutorService executor;
+    private final Duration writeTimeout;
     private final Map<String, TaskRunnable<?>> tasks = new ConcurrentHashMap<>();
 
     public CoordinatorExecutorImpl(
         LogContext logContext,
         TopicPartition shard,
         CoordinatorRuntime<S, U> runtime,
-        ExecutorService executor
+        ExecutorService executor,
+        Duration writeTimeout
     ) {
         this.log = logContext.logger(CoordinatorExecutorImpl.class);
         this.shard = shard;
         this.runtime = runtime;
         this.executor = executor;
+        this.writeTimeout = writeTimeout;
     }
 
     private <R> TaskResult<R> executeTask(TaskRunnable<R> task) {
@@ -79,6 +83,7 @@ public class CoordinatorExecutorImpl<S extends CoordinatorShard<U>, U> implement
             runtime.scheduleWriteOperation(
                 key,
                 shard,
+                writeTimeout,
                 coordinator -> {
                     // If the task associated with the key is not us, it means
                     // that the task was either replaced or cancelled. We stop.
diff --git a/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntime.java b/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntime.java
index 25f1e021d3..92afee2cc7 100644
--- a/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntime.java
+++ b/coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntime.java
@@ -113,7 +113,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
         private CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier;
         private Time time = Time.SYSTEM;
         private Timer timer;
-        private Duration writeTimeout;
+        private Duration defaultWriteTimeout;
         private CoordinatorRuntimeMetrics runtimeMetrics;
         private CoordinatorMetrics coordinatorMetrics;
         private Serializer<U> serializer;
@@ -162,8 +162,8 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
             return this;
         }
 
-        public Builder<S, U> withWriteTimeout(Duration writeTimeout) {
-            this.writeTimeout = writeTimeout;
+        public Builder<S, U> withDefaultWriteTimeOut(Duration defaultWriteTimeout) {
+            this.defaultWriteTimeout = defaultWriteTimeout;
             return this;
         }
 
@@ -246,7 +246,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
                 coordinatorShardBuilderSupplier,
                 time,
                 timer,
-                writeTimeout,
+                defaultWriteTimeout,
                 runtimeMetrics,
                 coordinatorMetrics,
                 serializer,
@@ -378,7 +378,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
                 @Override
                 public void run() {
                     String eventName = "Timeout(tp=" + tp + ", key=" + key + ")";
-                    CoordinatorWriteEvent<Void> event = new CoordinatorWriteEvent<>(eventName, tp, writeTimeout, coordinator -> {
+                    CoordinatorWriteEvent<Void> event = new CoordinatorWriteEvent<>(eventName, tp, defaultWriteTimeout, coordinator -> {
                         log.debug("Executing write event {} for timer {}.", eventName, key);
 
                         // If the task is different, it means that the timer has been
@@ -644,7 +644,8 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
                 logContext,
                 tp,
                 CoordinatorRuntime.this,
-                executorService
+                executorService,
+                defaultWriteTimeout
             );
             this.bufferSupplier = new BufferSupplier.GrowableBufferSupplier();
             this.cachedBufferSize = new AtomicLong(0);
@@ -2015,7 +2016,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
     /**
      * The write operation timeout
      */
-    private final Duration writeTimeout;
+    private final Duration defaultWriteTimeout;
 
     /**
      * The coordinators keyed by topic partition.
@@ -2102,7 +2103,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
      * @param coordinatorShardBuilderSupplier   The coordinator builder.
      * @param time                              The system time.
      * @param timer                             The system timer.
-     * @param writeTimeout                      The write operation timeout.
+     * @param defaultWriteTimeout               The write operation timeout.
      * @param runtimeMetrics                    The runtime metrics.
      * @param coordinatorMetrics                The coordinator metrics.
      * @param serializer                        The serializer.
@@ -2121,7 +2122,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
         CoordinatorShardBuilderSupplier<S, U> coordinatorShardBuilderSupplier,
         Time time,
         Timer timer,
-        Duration writeTimeout,
+        Duration defaultWriteTimeout,
         CoordinatorRuntimeMetrics runtimeMetrics,
         CoordinatorMetrics coordinatorMetrics,
         Serializer<U> serializer,
@@ -2134,7 +2135,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
         this.log = logContext.logger(CoordinatorRuntime.class);
         this.time = time;
         this.timer = timer;
-        this.writeTimeout = writeTimeout;
+        this.defaultWriteTimeout = defaultWriteTimeout;
         this.coordinators = new ConcurrentHashMap<>();
         this.processor = processor;
         this.partitionWriter = partitionWriter;
@@ -2249,6 +2250,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
      *
      * @param name      The name of the write operation.
      * @param tp        The address of the coordinator (aka its topic-partitions).
+     * @param timeout   The write operation timeout.
      * @param op        The write operation.
      *
      * @return A future that will be completed with the result of the write operation
@@ -2259,11 +2261,12 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
     public <T> CompletableFuture<T> scheduleWriteOperation(
         String name,
         TopicPartition tp,
+        Duration timeout,
         CoordinatorWriteOperation<S, T, U> op
     ) {
         throwIfNotRunning();
         log.debug("Scheduled execution of write operation {}.", name);
-        CoordinatorWriteEvent<T> event = new CoordinatorWriteEvent<>(name, tp, writeTimeout, op);
+        CoordinatorWriteEvent<T> event = new CoordinatorWriteEvent<>(name, tp, timeout, op);
         enqueueLast(event);
         return event.future;
     }
@@ -2272,6 +2275,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
      * Schedule a write operation for each coordinator.
      *
      * @param name      The name of the write operation.
+     * @param timeout   The write operation timeout.
      * @param op        The write operation.
      *
      * @return A list of futures where each future will be completed with the result of the write operation
@@ -2281,6 +2285,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
      */
     public <T> List<CompletableFuture<T>> scheduleWriteAllOperation(
         String name,
+        Duration timeout,
         CoordinatorWriteOperation<S, T, U> op
     ) {
         throwIfNotRunning();
@@ -2288,7 +2293,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
         return coordinators
             .keySet()
             .stream()
-            .map(tp -> scheduleWriteOperation(name, tp, op))
+            .map(tp -> scheduleWriteOperation(name, tp, timeout, op))
             .collect(Collectors.toList());
     }
 
@@ -2300,6 +2305,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
      * @param transactionalId   The transactional id.
      * @param producerId        The producer id.
      * @param producerEpoch     The producer epoch.
+     * @param timeout           The write operation timeout.
      * @param op                The write operation.
      * @param apiVersion        The Version of the Txn_Offset_Commit request
      *
@@ -2314,6 +2320,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
         String transactionalId,
         long producerId,
         short producerEpoch,
+        Duration timeout,
         CoordinatorWriteOperation<S, T, U> op,
         int apiVersion
     ) {
@@ -2333,7 +2340,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
                 producerId,
                 producerEpoch,
                 verificationGuard,
-                writeTimeout,
+                timeout,
                 op
             );
             enqueueLast(event);
@@ -2362,7 +2369,8 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
         short producerEpoch,
         int coordinatorEpoch,
         TransactionResult result,
-        short transactionVersion
+        short transactionVersion,
+        Duration timeout
     ) {
         throwIfNotRunning();
         log.debug("Scheduled execution of transaction completion for {} with producer id={}, producer epoch={}, " +
@@ -2375,7 +2383,7 @@ public class CoordinatorRuntime<S extends CoordinatorShard<U>, U> implements Aut
             coordinatorEpoch,
             result,
             transactionVersion,
-            writeTimeout
+            timeout
         );
         enqueueLast(event);
         return event.future;
diff --git a/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImplTest.java b/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImplTest.java
index 5a64a84d23..b2a82a6d70 100644
--- a/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImplTest.java
+++ b/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorExecutorImplTest.java
@@ -60,17 +60,19 @@ public class CoordinatorExecutorImplTest {
             LOG_CONTEXT,
             SHARD_PARTITION,
             runtime,
-            executorService
+            executorService,
+            WRITE_TIMEOUT
         );
 
         when(runtime.scheduleWriteOperation(
             eq(TASK_KEY),
             eq(SHARD_PARTITION),
+            eq(WRITE_TIMEOUT),
             any()
         )).thenAnswer(args -> {
             assertTrue(executor.isScheduled(TASK_KEY));
             CoordinatorRuntime.CoordinatorWriteOperation<CoordinatorShard<String>, Void, String> op =
-                args.getArgument(2);
+                args.getArgument(3);
             assertEquals(
                 new CoordinatorResult<>(List.of("record"), null),
                 op.generateRecordsAndResult(coordinatorShard)
@@ -118,16 +120,18 @@ public class CoordinatorExecutorImplTest {
             LOG_CONTEXT,
             SHARD_PARTITION,
             runtime,
-            executorService
+            executorService,
+            WRITE_TIMEOUT
         );
 
         when(runtime.scheduleWriteOperation(
             eq(TASK_KEY),
             eq(SHARD_PARTITION),
+            eq(WRITE_TIMEOUT),
             any()
         )).thenAnswer(args -> {
             CoordinatorRuntime.CoordinatorWriteOperation<CoordinatorShard<String>, Void, String> op =
-                args.getArgument(2);
+                args.getArgument(3);
             assertEquals(
                 new CoordinatorResult<>(List.of(), null),
                 op.generateRecordsAndResult(coordinatorShard)
@@ -174,7 +178,8 @@ public class CoordinatorExecutorImplTest {
             LOG_CONTEXT,
             SHARD_PARTITION,
             runtime,
-            executorService
+            executorService,
+            WRITE_TIMEOUT
         );
 
         when(executorService.submit(any(Runnable.class))).thenAnswer(args -> {
@@ -218,19 +223,21 @@ public class CoordinatorExecutorImplTest {
             LOG_CONTEXT,
             SHARD_PARTITION,
             runtime,
-            executorService
+            executorService,
+            WRITE_TIMEOUT
         );
 
         when(runtime.scheduleWriteOperation(
             eq(TASK_KEY),
             eq(SHARD_PARTITION),
+            eq(WRITE_TIMEOUT),
             any()
         )).thenAnswer(args -> {
             // Cancel the task before running the write operation.
             executor.cancel(TASK_KEY);
 
             CoordinatorRuntime.CoordinatorWriteOperation<CoordinatorShard<String>, Void, String> op =
-                args.getArgument(2);
+                args.getArgument(3);
             Throwable ex = assertThrows(RejectedExecutionException.class, () -> op.generateRecordsAndResult(coordinatorShard));
             return FutureUtils.failedFuture(ex);
         });
@@ -271,12 +278,14 @@ public class CoordinatorExecutorImplTest {
             LOG_CONTEXT,
             SHARD_PARTITION,
             runtime,
-            executorService
+            executorService,
+            WRITE_TIMEOUT
         );
 
         when(runtime.scheduleWriteOperation(
             eq(TASK_KEY),
             eq(SHARD_PARTITION),
+            eq(WRITE_TIMEOUT),
             any()
         )).thenReturn(FutureUtils.failedFuture(new Throwable("Oh no!")));
 
@@ -318,7 +327,8 @@ public class CoordinatorExecutorImplTest {
             LOG_CONTEXT,
             SHARD_PARTITION,
             runtime,
-            executorService
+            executorService,
+            WRITE_TIMEOUT
         );
 
         List<CoordinatorRuntime.CoordinatorWriteOperation<CoordinatorShard<String>, Void, String>> writeOperations = new ArrayList<>();
@@ -326,9 +336,10 @@ public class CoordinatorExecutorImplTest {
         when(runtime.scheduleWriteOperation(
             anyString(),
             eq(SHARD_PARTITION),
+            eq(WRITE_TIMEOUT),
             any()
         )).thenAnswer(args -> {
-            writeOperations.add(args.getArgument(2));
+            writeOperations.add(args.getArgument(3));
             CompletableFuture<Void> writeFuture = new CompletableFuture<>();
             writeFutures.add(writeFuture);
             return writeFuture;
diff --git a/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntimeTest.java b/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntimeTest.java
index ff1a77a2c4..00609c94b4 100644
--- a/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntimeTest.java
+++ b/coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntimeTest.java
@@ -119,7 +119,7 @@ public class CoordinatorRuntimeTest {
                 .withTime(timer.time())
                 .withTimer(timer)
                 .withLoader(loader)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
                 .withCoordinatorShardBuilderSupplier(supplier)
@@ -192,7 +192,7 @@ public class CoordinatorRuntimeTest {
                 .withTime(timer.time())
                 .withTimer(timer)
                 .withLoader(loader)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
                 .withCoordinatorShardBuilderSupplier(supplier)
@@ -245,7 +245,7 @@ public class CoordinatorRuntimeTest {
                 .withTime(timer.time())
                 .withTimer(timer)
                 .withLoader(loader)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
                 .withCoordinatorShardBuilderSupplier(supplier)
@@ -302,7 +302,7 @@ public class CoordinatorRuntimeTest {
                 .withTime(timer.time())
                 .withTimer(timer)
                 .withLoader(loader)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
                 .withCoordinatorShardBuilderSupplier(supplier)
@@ -375,7 +375,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -432,7 +432,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -489,7 +489,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -534,7 +534,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -585,7 +585,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -641,7 +641,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -669,7 +669,7 @@ public class CoordinatorRuntimeTest {
         CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
 
@@ -682,11 +682,12 @@ public class CoordinatorRuntimeTest {
             (short) 50,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            DEFAULT_WRITE_TIMEOUT
         );
 
         // Write #2 but without any records.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of(), "response2")
         );
 
@@ -730,7 +731,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -752,7 +753,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1")
         );
 
@@ -773,7 +774,7 @@ public class CoordinatorRuntimeTest {
         ), writer.entries(TP));
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record3"), "response2"));
 
         // Verify that the write is not committed yet.
@@ -794,7 +795,7 @@ public class CoordinatorRuntimeTest {
         ), writer.entries(TP));
 
         // Write #3 but without any records.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of(), "response3"));
 
         // Verify that the write is not committed yet.
@@ -837,7 +838,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(3L), ctx.coordinator.snapshotRegistry().epochsList());
 
         // Write #4 but without records.
-        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP,
+        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of(), "response4"));
 
         // It is completed immediately because the state is fully committed.
@@ -853,7 +854,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -867,7 +868,7 @@ public class CoordinatorRuntimeTest {
 
         // Scheduling a write fails with a NotCoordinatorException because the coordinator
         // does not exist.
-        CompletableFuture<String> write = runtime.scheduleWriteOperation("write", TP,
+        CompletableFuture<String> write = runtime.scheduleWriteOperation("write", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of(), "response1"));
         assertFutureThrows(NotCoordinatorException.class, write);
     }
@@ -879,7 +880,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -896,7 +897,7 @@ public class CoordinatorRuntimeTest {
 
         // Scheduling a write that fails when the operation is called. The exception
         // is used to complete the future.
-        CompletableFuture<String> write = runtime.scheduleWriteOperation("write", TP, state -> {
+        CompletableFuture<String> write = runtime.scheduleWriteOperation("write", TP, DEFAULT_WRITE_TIMEOUT, state -> {
             throw new KafkaException("error");
         });
         assertFutureThrows(KafkaException.class, write);
@@ -909,7 +910,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -951,7 +952,7 @@ public class CoordinatorRuntimeTest {
         );
 
         // Write. It should fail.
-        CompletableFuture<String> write = runtime.scheduleWriteOperation("write", TP,
+        CompletableFuture<String> write = runtime.scheduleWriteOperation("write", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"));
         assertFutureThrows(IllegalArgumentException.class, write);
 
@@ -971,7 +972,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -993,7 +994,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());
 
         // Write #1. It should succeed and be applied to the coordinator.
-        runtime.scheduleWriteOperation("write#1", TP,
+        runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"));
 
         // Verify that the state has been updated.
@@ -1004,7 +1005,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #2. It should fail because the writer is configured to only
         // accept 1 write.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record3", "record4", "record5"), "response2"));
         assertFutureThrows(KafkaException.class, write2);
 
@@ -1025,7 +1026,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1047,10 +1048,10 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());
 
         // Write #1. We should get a TimeoutException because the HWM will not advance.
-        CompletableFuture<String> timedOutWrite = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> timedOutWrite = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(3),
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"));
 
-        timer.advanceClock(DEFAULT_WRITE_TIMEOUT.toMillis() + 1);
+        timer.advanceClock(4);
 
         assertFutureThrows(org.apache.kafka.common.errors.TimeoutException.class, timedOutWrite);
     }
@@ -1064,7 +1065,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1087,7 +1088,7 @@ public class CoordinatorRuntimeTest {
 
         // Writes.
         AtomicInteger cnt = new AtomicInteger(0);
-        List<CompletableFuture<List<String>>> writes = runtime.scheduleWriteAllOperation("write", state -> {
+        List<CompletableFuture<List<String>>> writes = runtime.scheduleWriteAllOperation("write", DEFAULT_WRITE_TIMEOUT, state -> {
             int counter = cnt.getAndIncrement();
             return new CoordinatorResult<>(
                 List.of("record#" + counter),
@@ -1137,7 +1138,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1175,6 +1176,7 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             (short) 50,
+            Duration.ofMillis(5000),
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
@@ -1233,7 +1235,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1267,6 +1269,7 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             (short) 50,
+            Duration.ofMillis(5000),
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
@@ -1296,7 +1299,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1325,6 +1328,7 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             transactionalEpoch,
+            DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
@@ -1359,7 +1363,8 @@ public class CoordinatorRuntimeTest {
             markerEpoch,
             10,
             result,
-            transactionVersion
+            transactionVersion,
+            DEFAULT_WRITE_TIMEOUT
         );
 
         // Verify that the completion is not committed yet.
@@ -1423,7 +1428,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1453,7 +1458,8 @@ public class CoordinatorRuntimeTest {
             (short) 5,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            Duration.ofMillis(3)
         );
 
         // Verify that the state has been updated.
@@ -1462,7 +1468,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(0L, 1L), ctx.coordinator.snapshotRegistry().epochsList());
 
         // Advance clock to timeout Complete #1.
-        timer.advanceClock(DEFAULT_WRITE_TIMEOUT.toMillis() + 1);
+        timer.advanceClock(4);
 
         assertFutureThrows(org.apache.kafka.common.errors.TimeoutException.class, timedOutCompletion);
 
@@ -1485,7 +1491,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1513,6 +1519,7 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             (short) 5,
+            DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
@@ -1532,7 +1539,8 @@ public class CoordinatorRuntimeTest {
             (short) 5,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            DEFAULT_WRITE_TIMEOUT
         );
         assertFutureThrows(KafkaException.class, complete1);
 
@@ -1552,7 +1560,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1599,6 +1607,7 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             (short) 5,
+            DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
@@ -1622,7 +1631,8 @@ public class CoordinatorRuntimeTest {
             (short) 5,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            DEFAULT_WRITE_TIMEOUT
         );
         assertFutureThrows(IllegalArgumentException.class, complete1);
 
@@ -1652,7 +1662,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1680,6 +1690,7 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             transactionalEpoch,
+            DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
@@ -1700,7 +1711,8 @@ public class CoordinatorRuntimeTest {
             markerEpoch,
             10,
             TransactionResult.COMMIT,
-            transactionVersion
+            transactionVersion,
+            DEFAULT_WRITE_TIMEOUT
         );
 
         // Verify that InvalidProducerEpochException is thrown.
@@ -1737,7 +1749,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1758,11 +1770,11 @@ public class CoordinatorRuntimeTest {
         assertEquals(0, ctx.coordinator.lastCommittedOffset());
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"));
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record3", "record4"), "response2"));
 
         // Commit write #1.
@@ -1797,7 +1809,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -1824,7 +1836,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1845,11 +1857,11 @@ public class CoordinatorRuntimeTest {
         assertEquals(0, ctx.coordinator.lastCommittedOffset());
 
         // Write #1.
-        runtime.scheduleWriteOperation("write#1", TP,
+        runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"));
 
         // Write #2.
-        runtime.scheduleWriteOperation("write#2", TP,
+        runtime.scheduleWriteOperation("write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record3", "record4"), "response2"));
 
         // Commit write #1.
@@ -1872,7 +1884,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -1894,11 +1906,11 @@ public class CoordinatorRuntimeTest {
         runtime.scheduleLoadOperation(coordinator2, 10);
 
         // Writes
-        runtime.scheduleWriteOperation("write#0", coordinator0,
+        runtime.scheduleWriteOperation("write#0", coordinator0, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record0"), "response0"));
-        runtime.scheduleWriteOperation("write#1", coordinator1,
+        runtime.scheduleWriteOperation("write#1", coordinator1, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1"), "response1"));
-        runtime.scheduleWriteOperation("write#2", coordinator2,
+        runtime.scheduleWriteOperation("write#2", coordinator2, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record2"), "response2"));
 
         // Commit writes.
@@ -1928,7 +1940,7 @@ public class CoordinatorRuntimeTest {
                 .withTime(timer.time())
                 .withTimer(timer)
                 .withLoader(loader)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
                 .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())
@@ -1948,11 +1960,11 @@ public class CoordinatorRuntimeTest {
         assertEquals(0, ctx.coordinator.lastCommittedOffset());
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1"));
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record3", "record4"), "response2"));
 
         // Writes are inflight.
@@ -2002,7 +2014,7 @@ public class CoordinatorRuntimeTest {
                 .withTime(timer.time())
                 .withTimer(timer)
                 .withLoader(loader)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
                 .withCoordinatorShardBuilderSupplier(supplier)
@@ -2067,7 +2079,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(30))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2125,7 +2137,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2202,7 +2214,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2276,7 +2288,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2338,7 +2350,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2414,7 +2426,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2459,7 +2471,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2512,7 +2524,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(new MockPartitionWriter())
@@ -2571,7 +2583,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(loader)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -2644,7 +2656,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader(
                     new CoordinatorLoader.LoadSummary(
                         startTimeMs,
@@ -2703,7 +2715,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(Time.SYSTEM)
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader(
                     new CoordinatorLoader.LoadSummary(
                         1000,
@@ -2763,7 +2775,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(Time.SYSTEM)
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader(
                     new CoordinatorLoader.LoadSummary(
                         1000,
@@ -2817,7 +2829,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -2837,13 +2849,13 @@ public class CoordinatorRuntimeTest {
         processor.poll();
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
         processor.poll();
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record2"), "response2")
         );
         processor.poll();
@@ -2893,7 +2905,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -2910,7 +2922,7 @@ public class CoordinatorRuntimeTest {
         runtime.scheduleLoadOperation(TP, 10);
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
 
@@ -2923,16 +2935,17 @@ public class CoordinatorRuntimeTest {
             (short) 50,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            DEFAULT_WRITE_TIMEOUT
         );
 
         // Write #2 but without any records.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of(), "response2")
         );
 
         // Write #3, also without any records. Should complete together with write #2.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of(), "response3")
         );
 
@@ -2972,7 +2985,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -2992,13 +3005,13 @@ public class CoordinatorRuntimeTest {
         processor.poll();
 
         // Write#1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("Write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("Write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
         processor.poll();
 
         // Write#2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("Write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("Write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(List.of("record2"), "response2")
         );
         processor.poll();
@@ -3048,7 +3061,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -3075,7 +3088,8 @@ public class CoordinatorRuntimeTest {
             (short) 50,
             1,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            DEFAULT_WRITE_TIMEOUT
         );
         processor.poll();
 
@@ -3119,7 +3133,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -3150,7 +3164,7 @@ public class CoordinatorRuntimeTest {
         }
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(records, "response1")
         );
 
@@ -3173,7 +3187,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(mockWriter)
@@ -3198,7 +3212,7 @@ public class CoordinatorRuntimeTest {
         List<String> largeRecords = List.of("A".repeat(100 * 1024 * 1024));
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(largeRecords, "response1", null, true, false)
         );
 
@@ -3222,7 +3236,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(mockWriter)
@@ -3247,7 +3261,7 @@ public class CoordinatorRuntimeTest {
         List<String> records = List.of("A".repeat(INITIAL_BUFFER_SIZE + 1024));
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(records, "response1")
         );
 
@@ -3275,7 +3289,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(DEFAULT_WRITE_TIMEOUT)
+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(mockWriter)
@@ -3299,7 +3313,7 @@ public class CoordinatorRuntimeTest {
         List<String> records = List.of("A".repeat(INITIAL_BUFFER_SIZE + 1024));
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(records, "response1")
         );
 
@@ -3319,7 +3333,7 @@ public class CoordinatorRuntimeTest {
         when(maxBufferSizeSupplierMock.get()).thenReturn(batchSize - 66);
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(records, "response2")
         );
         assertFalse(write2.isCompletedExceptionally());
@@ -3329,7 +3343,7 @@ public class CoordinatorRuntimeTest {
         verify(runtimeMetrics, times(1)).recordBufferCacheDiscarded();
 
         // Write #3.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, DEFAULT_WRITE_TIMEOUT,
             state -> new CoordinatorResult<>(records, "response3")
         );
         assertFalse(write3.isCompletedExceptionally());
@@ -3348,7 +3362,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -3384,7 +3398,7 @@ public class CoordinatorRuntimeTest {
         }).collect(Collectors.toList());
 
         // Write #1 with two records.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(0, 2), "response1")
         );
 
@@ -3405,7 +3419,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(), writer.entries(TP));
 
         // Write #2 with one record.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(2, 3), "response2")
         );
 
@@ -3425,7 +3439,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #3 with one record. This one cannot go into the existing batch
         // so the existing batch should be flushed and a new one should be created.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(3, 4), "response3")
         );
 
@@ -3484,7 +3498,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -3520,7 +3534,7 @@ public class CoordinatorRuntimeTest {
         }).collect(Collectors.toList());
 
         // Write all the records.
-        CompletableFuture<String> write = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records, "response1")
         );
 
@@ -3537,7 +3551,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -3573,15 +3587,15 @@ public class CoordinatorRuntimeTest {
         }).collect(Collectors.toList());
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(0, 1), "response1"));
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(1, 2), "response2"));
 
         // Write #3.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(2, 3), "response3"));
 
         // Verify the state.
@@ -3597,7 +3611,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #4. This write cannot make it in the current batch. So the current batch
         // is flushed. It will fail. So we expect all writes to fail.
-        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP,
+        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(3, 4), "response4"));
 
         // Verify the futures.
@@ -3624,7 +3638,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -3686,7 +3700,7 @@ public class CoordinatorRuntimeTest {
         }).collect(Collectors.toList());
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(0, 1), "response1"));
 
         // Verify the state.
@@ -3699,7 +3713,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(), writer.entries(TP));
 
         // Write #2. It should fail.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(1, 2), "response2"));
 
         // Verify the futures.
@@ -3723,7 +3737,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -3747,7 +3761,7 @@ public class CoordinatorRuntimeTest {
         assertNull(ctx.currentBatch);
 
         // Write #1 with one record.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record#1"), "response1")
         );
 
@@ -3769,6 +3783,7 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             (short) 50,
+            Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record#2"), "response2"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
@@ -3789,7 +3804,7 @@ public class CoordinatorRuntimeTest {
         ), writer.entries(TP));
 
         // Write #3 with one record.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record#3"), "response3")
         );
 
@@ -3816,7 +3831,8 @@ public class CoordinatorRuntimeTest {
             (short) 50,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            DEFAULT_WRITE_TIMEOUT
         );
 
         // Verify that the completion is not committed yet.
@@ -3857,7 +3873,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -3884,12 +3900,12 @@ public class CoordinatorRuntimeTest {
         assertNull(ctx.currentBatch);
 
         // Write #1.
-        runtime.scheduleWriteOperation("write#1", TP,
+        runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record1", "record2"), "response1")
         );
 
         // Write #2.
-        runtime.scheduleWriteOperation("write#2", TP,
+        runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record3"), "response2")
         );
 
@@ -3966,7 +3982,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -4005,22 +4021,22 @@ public class CoordinatorRuntimeTest {
         }).collect(Collectors.toList());
 
         // Write #1.
-        runtime.scheduleWriteOperation("write#1", TP,
+        runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(0, 1), "response1")
         );
 
         // Write #2.
-        runtime.scheduleWriteOperation("write#2", TP,
+        runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(1, 2), "response2")
         );
 
         // Write #3.
-        runtime.scheduleWriteOperation("write#3", TP,
+        runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(2, 3), "response3")
         );
 
         // Write #4.
-        runtime.scheduleWriteOperation("write#4", TP,
+        runtime.scheduleWriteOperation("write#4", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(3, 4), "response4")
         );
 
@@ -4047,7 +4063,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(), writer.entries(TP));
 
         // Write #5.
-        runtime.scheduleWriteOperation("write#5", TP,
+        runtime.scheduleWriteOperation("write#5", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(4, 5), "response5")
         );
 
@@ -4156,7 +4172,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -4183,7 +4199,7 @@ public class CoordinatorRuntimeTest {
         assertNull(ctx.currentBatch);
 
         // Write #1.
-        runtime.scheduleWriteOperation("write#1", TP,
+        runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
 
@@ -4194,12 +4210,13 @@ public class CoordinatorRuntimeTest {
             "transactional-id",
             100L,
             (short) 50,
+            Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record2"), "response2"),
             TXN_OFFSET_COMMIT_LATEST_VERSION
         );
 
         // Write #3.
-        runtime.scheduleWriteOperation("write#2", TP,
+        runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record3"), "response3")
         );
 
@@ -4317,7 +4334,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(loader)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4357,20 +4374,20 @@ public class CoordinatorRuntimeTest {
         }).collect(Collectors.toList());
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(0, 1), "response1"));
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(1, 2), "response2"));
 
         // Write #3.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(2, 3), "response3"));
 
         // Write #4. This write cannot make it in the current batch. So the current batch
         // is flushed. It will fail. So we expect all writes to fail.
-        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP,
+        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(3, 4), "response4"));
 
         // Verify the futures.
@@ -4433,7 +4450,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(loader)
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4456,7 +4473,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(List.of(2L), ctx.coordinator.snapshotRegistry().epochsList());
 
         // Schedule a write operation that does not generate any records.
-        CompletableFuture<String> write = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of(), "response1"));
 
         // The write operation should not be done.
@@ -4483,7 +4500,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4520,14 +4537,14 @@ public class CoordinatorRuntimeTest {
 
         // Let's try to write all the records atomically (the default) to ensure
         // that it fails.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records, "write#1")
         );
 
         assertFutureThrows(RecordTooLargeException.class, write1);
 
         // Let's try to write the same records non-atomically.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records, "write#2", null, true, false)
         );
 
@@ -4593,7 +4610,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4634,7 +4651,7 @@ public class CoordinatorRuntimeTest {
         String record = new String(payload);
 
         // Let's write the first three records.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records, "write#1", null, true, false)
         );
 
@@ -4653,7 +4670,7 @@ public class CoordinatorRuntimeTest {
         // pending batch, allocate a new batch, and put the record into it.
         // Note that the batch will fail only when the batch is written because the
         // MemoryBatchBuilder always accept one record.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of(record), "write#2", null, true, false)
         );
 
@@ -4690,7 +4707,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4726,15 +4743,15 @@ public class CoordinatorRuntimeTest {
         }).collect(Collectors.toList());
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(0, 1), "response1", null, true, false));
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(1, 2), "response2", null, true, false));
 
         // Write #3.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(2, 3), "response3", null, true, false));
 
         // Verify the state.
@@ -4750,7 +4767,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #4. This write cannot make it in the current batch. So the current batch
         // is flushed. It will fail. So we expect all writes to fail.
-        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP,
+        CompletableFuture<String> write4 = runtime.scheduleWriteOperation("write#4", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(records.subList(3, 4), "response4", null, true, false));
 
         // Verify the futures.
@@ -4778,7 +4795,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4800,7 +4817,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #1, which fails.
         serializer.throwOnNextOperation();
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("1"), "response1"));
 
         // Write #1 should fail and leave an empty batch.
@@ -4808,7 +4825,7 @@ public class CoordinatorRuntimeTest {
         assertNotNull(ctx.currentBatch);
 
         // Write #2, with no records.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of(), "response2"));
 
         // Write #2 should not be attached to the empty batch.
@@ -4826,7 +4843,8 @@ public class CoordinatorRuntimeTest {
             (short) 50,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            DEFAULT_WRITE_TIMEOUT
         );
 
         // Verify that the completion is not committed yet.
@@ -4849,7 +4867,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4883,7 +4901,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #1 with two records.
         long firstBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records.subList(0, 2), "response1")
         );
 
@@ -4891,7 +4909,7 @@ public class CoordinatorRuntimeTest {
         assertNotNull(ctx.currentBatch);
 
         // Write #2 with one record.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records.subList(2, 3), "response2")
         );
 
@@ -4902,7 +4920,7 @@ public class CoordinatorRuntimeTest {
         // Write #3 with one record. This one cannot go into the existing batch
         // so the existing batch should be flushed and a new one should be created.
         long secondBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records.subList(3, 4), "response3")
         );
 
@@ -4963,7 +4981,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -4997,7 +5015,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #1 with two records.
         long firstBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records.subList(0, 2), "response1")
         );
 
@@ -5005,7 +5023,7 @@ public class CoordinatorRuntimeTest {
         assertNotNull(ctx.currentBatch);
 
         // Write #2 with one record.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records.subList(2, 3), "response2")
         );
 
@@ -5016,7 +5034,7 @@ public class CoordinatorRuntimeTest {
         // Write #3 with one record. This one cannot go into the existing batch
         // so the existing batch should be flushed and a new one should be created.
         long secondBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records.subList(3, 4), "response3")
         );
 
@@ -5074,7 +5092,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -5107,7 +5125,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #1 with the small records, batch will be about half full
         long firstBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records, "response1")
         );
 
@@ -5123,7 +5141,7 @@ public class CoordinatorRuntimeTest {
         // Write #2 with the large record. This record is too large to go into the previous batch
         // uncompressed but fits in a new buffer, so we should flush the previous batch and allocate
         // a new one.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(largeRecord, "response2")
         );
 
@@ -5161,7 +5179,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -5194,7 +5212,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #1 with the small records, batch will be about half full
         long firstBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records, "response1")
         );
 
@@ -5212,7 +5230,7 @@ public class CoordinatorRuntimeTest {
         // previous batch and successfully allocate a new batch for this record. The new batch
         // will also trigger an immediate flush.
         long secondBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(largeRecord, "response2")
         );
 
@@ -5248,7 +5266,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -5281,7 +5299,7 @@ public class CoordinatorRuntimeTest {
 
         // Write #1 with the small records, batch will be about half full
         long firstBatchTimestamp = timer.time().milliseconds();
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(records, "response1")
         );
 
@@ -5302,7 +5320,7 @@ public class CoordinatorRuntimeTest {
         // Write #2 with the large record. This record is too large to go into the previous batch
         // and is not compressible so it should be flushed. It is also too large to fit in a new batch
         // so the write should fail with RecordTooLargeException
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(50),
             state -> new CoordinatorResult<>(largeRecord, "response2")
         );
 
@@ -5341,7 +5359,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(writeTimeout)
+                .withDefaultWriteTimeOut(writeTimeout)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -5361,15 +5379,15 @@ public class CoordinatorRuntimeTest {
         processor.poll();
 
         // write#1 will be committed and update the high watermark. Record time spent in purgatory.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, writeTimeout,
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
         // write#2 will time out sitting in the purgatory. Record time spent in purgatory.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, writeTimeout,
             state -> new CoordinatorResult<>(List.of("record2"), "response2")
         );
         // write#3 will error while appending. Does not spend time in purgatory.
-        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP,
+        CompletableFuture<String> write3 = runtime.scheduleWriteOperation("write#3", TP, writeTimeout,
             state -> {
                 throw new KafkaException("write#3 failed.");
             });
@@ -5429,7 +5447,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(writeTimeout)
+                .withDefaultWriteTimeOut(writeTimeout)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -5449,7 +5467,7 @@ public class CoordinatorRuntimeTest {
         processor.poll();
 
         // write#1 will be committed and update the high watermark. Record time spent in purgatory.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, writeTimeout,
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
 
@@ -5498,7 +5516,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(writeTimeout)
+                .withDefaultWriteTimeOut(writeTimeout)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -5525,7 +5543,8 @@ public class CoordinatorRuntimeTest {
             (short) 50,
             1,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            writeTimeout
         );
         processor.poll();
 
@@ -5578,7 +5597,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(timer.time())
                 .withTimer(timer)
-                .withWriteTimeout(writeTimeout)
+                .withDefaultWriteTimeOut(writeTimeout)
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(processor)
                 .withPartitionWriter(writer)
@@ -5598,7 +5617,7 @@ public class CoordinatorRuntimeTest {
         processor.poll();
 
         // Schedule a write which schedules an async tasks.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, writeTimeout,
             state -> {
                 state.executor().schedule(
                     "write#1#task",
@@ -5661,7 +5680,7 @@ public class CoordinatorRuntimeTest {
             new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()
                 .withTime(clockTimer.time())
                 .withTimer(schedulerTimer)
-                .withWriteTimeout(Duration.ofMillis(30))
+                .withDefaultWriteTimeOut(Duration.ofMillis(20))
                 .withLoader(new MockCoordinatorLoader())
                 .withEventProcessor(new DirectEventProcessor())
                 .withPartitionWriter(writer)
@@ -5683,7 +5702,7 @@ public class CoordinatorRuntimeTest {
         assertNull(ctx.currentBatch);
 
         // Write #1.
-        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP,
+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation("write#1", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record1"), "response1")
         );
         assertFalse(write1.isDone());
@@ -5701,7 +5720,7 @@ public class CoordinatorRuntimeTest {
         assertEquals(2, schedulerTimer.size());
 
         // Write #2.
-        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP,
+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation("write#2", TP, Duration.ofMillis(20),
             state -> new CoordinatorResult<>(List.of("record2"), "response2")
         );
 
diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala
index 7afd5559b7..f4c23c401e 100644
--- a/core/src/main/scala/kafka/server/KafkaApis.scala
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala
@@ -72,6 +72,7 @@ import org.apache.kafka.server.transaction.AddPartitionsToTxnManager
 import org.apache.kafka.storage.internals.log.AppendOrigin
 import org.apache.kafka.storage.log.metrics.BrokerTopicStats
 
+import java.time.Duration
 import java.util
 import java.util.concurrent.atomic.AtomicInteger
 import java.util.concurrent.{CompletableFuture, ConcurrentHashMap}
@@ -1791,7 +1792,8 @@ class KafkaApis(val requestChannel: RequestChannel,
               marker.producerEpoch,
               marker.coordinatorEpoch,
               marker.transactionResult,
-              markerTransactionVersion
+              markerTransactionVersion,
+              Duration.ofMillis(config.requestTimeoutMs.toLong)
             ).whenComplete { (_, exception) =>
               val error = if (exception == null) {
                 Errors.NONE
diff --git a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala
index b1d68c8146..34472d4ca6 100644
--- a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala
+++ b/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala
@@ -114,6 +114,7 @@ import java.lang.{Byte => JByte}
 import java.net.InetAddress
 import java.nio.ByteBuffer
 import java.nio.charset.StandardCharsets
+import java.time.Duration
 import java.util
 import java.util.concurrent.{CompletableFuture, TimeUnit}
 import java.util.function.Consumer
@@ -3122,7 +3123,8 @@ class KafkaApisTest extends Logging {
       ArgumentMatchers.eq(1.toShort),
       ArgumentMatchers.eq(0),
       ArgumentMatchers.eq(TransactionResult.COMMIT),
-      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel())
+      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel()),
+      any()
     )).thenReturn(CompletableFuture.completedFuture[Void](null))
 
     kafkaApis = createKafkaApis()
@@ -3273,7 +3275,8 @@ class KafkaApisTest extends Logging {
       ArgumentMatchers.eq(1.toShort),
       ArgumentMatchers.eq(0),
       ArgumentMatchers.eq(TransactionResult.COMMIT),
-      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel())
+      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel()),
+      ArgumentMatchers.eq(Duration.ofMillis(ServerConfigs.REQUEST_TIMEOUT_MS_DEFAULT))
     )).thenReturn(CompletableFuture.completedFuture[Void](null))
 
     when(groupCoordinator.completeTransaction(
@@ -3282,7 +3285,8 @@ class KafkaApisTest extends Logging {
       ArgumentMatchers.eq(1.toShort),
       ArgumentMatchers.eq(0),
       ArgumentMatchers.eq(TransactionResult.ABORT),
-      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel())
+      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel()),
+      ArgumentMatchers.eq(Duration.ofMillis(ServerConfigs.REQUEST_TIMEOUT_MS_DEFAULT))
     )).thenReturn(CompletableFuture.completedFuture[Void](null))
 
     val entriesPerPartition: ArgumentCaptor[Map[TopicIdPartition, MemoryRecords]] =
@@ -3389,7 +3393,8 @@ class KafkaApisTest extends Logging {
       ArgumentMatchers.eq(1.toShort),
       ArgumentMatchers.eq(0),
       ArgumentMatchers.eq(TransactionResult.COMMIT),
-      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel())
+      ArgumentMatchers.eq(TransactionVersion.TV_2.featureLevel()),
+      ArgumentMatchers.eq(Duration.ofMillis(ServerConfigs.REQUEST_TIMEOUT_MS_DEFAULT))
     )).thenReturn(FutureUtils.failedFuture[Void](error.exception()))
     kafkaApis = createKafkaApis()
     kafkaApis.handleWriteTxnMarkersRequest(requestChannelRequest, RequestLocal.noCaching)
diff --git a/docs/operations/monitoring.md b/docs/operations/monitoring.md
index 746a74ac6b..21065cecab 100644
--- a/docs/operations/monitoring.md
+++ b/docs/operations/monitoring.md
@@ -2045,7 +2045,7 @@ Average Poll Idle Ratio
 </td>  
 <td>
 
-The average fraction of time the client's poll() is idle as opposed to waiting for the user code to process records.
+The ratio of time the Raft IO thread is idle as opposed to doing work (e.g. handling requests or replicating from the leader)
 </td>  
 <td>
 
diff --git a/docs/security/authentication-using-sasl.md b/docs/security/authentication-using-sasl.md
index cfcb24d06f..9cb55b3dd9 100644
--- a/docs/security/authentication-using-sasl.md
+++ b/docs/security/authentication-using-sasl.md
@@ -45,10 +45,11 @@ Brokers may also configure JAAS using the broker configuration property `sasl.ja
                 user_admin="admin-secret" \
                 user_alice="alice-secret";
 
-If JAAS configuration is defined at different levels, the order of precedence used is: 
-        * Broker configuration property `listener.name.{listenerName}.{saslMechanism}.sasl.jaas.config`
-        * `{listenerName}.KafkaServer` section of static JAAS configuration
-        * `KafkaServer` section of static JAAS configuration
+If JAAS configuration is defined at different levels, the order of precedence used is:
+
+* Broker configuration property `listener.name.{listenerName}.{saslMechanism}.sasl.jaas.config`
+* `{listenerName}.KafkaServer` section of static JAAS configuration
+* `KafkaServer` section of static JAAS configuration
 
 See GSSAPI (Kerberos), PLAIN, SCRAM, or non-production/production OAUTHBEARER for example broker configurations.
 
@@ -64,20 +65,25 @@ See GSSAPI (Kerberos), PLAIN, SCRAM, or non-production/production OAUTHBEARER fo
 
 ##### JAAS configuration using static config file
 
-To configure SASL authentication on the clients using static JAAS config file: 
-           1. Add a JAAS config file with a client login section named `KafkaClient`. Configure a login module in `KafkaClient` for the selected mechanism as described in the examples for setting up GSSAPI (Kerberos), PLAIN, SCRAM, or non-production/production OAUTHBEARER. For example, GSSAPI credentials may be configured as: 
-                  
-                  KafkaClient {
-                      com.sun.security.auth.module.Krb5LoginModule required
-                      useKeyTab=true
-                      storeKey=true
-                      keyTab="/etc/security/keytabs/kafka_client.keytab"
-                      principal="kafka-client-1@EXAMPLE.COM";
-                  };
-
-           2. Pass the JAAS config file location as JVM parameter to each client JVM. For example: 
-                  
-                  -Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf
+To configure SASL authentication on the clients using static JAAS config file:
+
+1. Add a JAAS config file with a client login section named `KafkaClient`. Configure a login module in `KafkaClient` for the selected mechanism as described in the examples for setting up GSSAPI (Kerberos), PLAIN, SCRAM, or non-production/production OAUTHBEARER. For example, GSSAPI credentials may be configured as:
+
+   ```
+   KafkaClient {
+       com.sun.security.auth.module.Krb5LoginModule required
+       useKeyTab=true
+       storeKey=true
+       keyTab="/etc/security/keytabs/kafka_client.keytab"
+       principal="kafka-client-1@EXAMPLE.COM";
+   };
+   ```
+
+2. Pass the JAAS config file location as JVM parameter to each client JVM. For example:
+
+   ```
+   -Djava.security.auth.login.config=/etc/kafka/kafka_client_jaas.conf
+   ```
 
 ### SASL configuration
 
@@ -85,23 +91,28 @@ SASL may be used with PLAINTEXT or SSL as the transport layer using the security
 
 #### SASL mechanisms
 
-Kafka supports the following SASL mechanisms: 
-        * GSSAPI (Kerberos)
-        * PLAIN
-        * SCRAM-SHA-256
-        * SCRAM-SHA-512
-        * OAUTHBEARER
+Kafka supports the following SASL mechanisms:
+
+* GSSAPI (Kerberos)
+* PLAIN
+* SCRAM-SHA-256
+* SCRAM-SHA-512
+* OAUTHBEARER
 #### SASL configuration for Kafka brokers
 
-        1. Configure a SASL port in server.properties, by adding at least one of SASL_PLAINTEXT or SASL_SSL to the _listeners_ parameter, which contains one or more comma-separated values: 
-               
-               listeners=SASL_PLAINTEXT://host.name:port
+1. Configure a SASL port in server.properties, by adding at least one of SASL_PLAINTEXT or SASL_SSL to the _listeners_ parameter, which contains one or more comma-separated values:
 
-If you are only configuring a SASL port (or if you want the Kafka brokers to authenticate each other using SASL) then make sure you set the same SASL protocol for inter-broker communication: 
-               
-               security.inter.broker.protocol=SASL_PLAINTEXT (or SASL_SSL)
+   ```
+   listeners=SASL_PLAINTEXT://host.name:port
+   ```
 
-        2. Select one or more supported mechanisms to enable in the broker and follow the steps to configure SASL for the mechanism. To enable multiple mechanisms in the broker, follow the steps here.
+   If you are only configuring a SASL port (or if you want the Kafka brokers to authenticate each other using SASL) then make sure you set the same SASL protocol for inter-broker communication:
+
+   ```
+   security.inter.broker.protocol=SASL_PLAINTEXT (or SASL_SSL)
+   ```
+
+2. Select one or more supported mechanisms to enable in the broker and follow the steps to configure SASL for the mechanism. To enable multiple mechanisms in the broker, follow the steps here.
 #### SASL configuration for Kafka clients
 
 SASL authentication is only supported for the new Java Kafka producer and consumer, the older API is not supported.
@@ -114,50 +125,64 @@ Note: When establishing connections to brokers via SASL, clients may perform a r
 
 #### Prerequisites
 
-        1. **Kerberos**  
-If your organization is already using a Kerberos server (for example, by using Active Directory), there is no need to install a new server just for Kafka. Otherwise you will need to install one, your Linux vendor likely has packages for Kerberos and a short guide on how to install and configure it ([Ubuntu](https://help.ubuntu.com/community/Kerberos), [Redhat](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/installing-kerberos.html)). Note that if you are using Oracle Java, you will need to download JCE policy files for your Java version and copy them to $JAVA_HOME/jre/lib/security.
-        2. **Create Kerberos Principals**  
-If you are using the organization's Kerberos or Active Directory server, ask your Kerberos administrator for a principal for each Kafka broker in your cluster and for every operating system user that will access Kafka with Kerberos authentication (via clients and tools).  
-If you have installed your own Kerberos, you will need to create these principals yourself using the following commands: 
-               
-               $ sudo /usr/sbin/kadmin.local -q 'addprinc -randkey kafka/{hostname}@{REALM}'
-               $ sudo /usr/sbin/kadmin.local -q "ktadd -k /etc/security/keytabs/{keytabname}.keytab kafka/{hostname}@{REALM}"
+1. **Kerberos**
+   If your organization is already using a Kerberos server (for example, by using Active Directory), there is no need to install a new server just for Kafka. Otherwise you will need to install one, your Linux vendor likely has packages for Kerberos and a short guide on how to install and configure it ([Ubuntu](https://help.ubuntu.com/community/Kerberos), [Redhat](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/installing-kerberos.html)). Note that if you are using Oracle Java, you will need to download JCE policy files for your Java version and copy them to $JAVA_HOME/jre/lib/security.
+
+2. **Create Kerberos Principals**
+   If you are using the organization's Kerberos or Active Directory server, ask your Kerberos administrator for a principal for each Kafka broker in your cluster and for every operating system user that will access Kafka with Kerberos authentication (via clients and tools).
 
-        3. **Make sure all hosts can be reachable using hostnames** \- it is a Kerberos requirement that all your hosts can be resolved with their FQDNs.
+   If you have installed your own Kerberos, you will need to create these principals yourself using the following commands:
+
+   ```
+   $ sudo /usr/sbin/kadmin.local -q 'addprinc -randkey kafka/{hostname}@{REALM}'
+   $ sudo /usr/sbin/kadmin.local -q "ktadd -k /etc/security/keytabs/{keytabname}.keytab kafka/{hostname}@{REALM}"
+   ```
+
+3. **Make sure all hosts can be reachable using hostnames** - it is a Kerberos requirement that all your hosts can be resolved with their FQDNs.
 #### Configuring Kafka Brokers
 
-        1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example (note that each broker should have its own keytab): 
-               
-               KafkaServer {
-                   com.sun.security.auth.module.Krb5LoginModule required
-                   useKeyTab=true
-                   storeKey=true
-                   keyTab="/etc/security/keytabs/kafka_server.keytab"
-                   principal="kafka/kafka1.hostname.com@EXAMPLE.COM";
-               };
+1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example (note that each broker should have its own keytab):
 
-`KafkaServer` section in the JAAS file tells the broker which principal to use and the location of the keytab where this principal is stored. It allows the broker to login using the keytab specified in this section. 
-        2. Pass the JAAS and optionally the krb5 file locations as JVM parameters to each Kafka broker (see [here](https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html) for more details): 
-               
-               -Djava.security.krb5.conf=/etc/kafka/krb5.conf
-               -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
+   ```
+   KafkaServer {
+       com.sun.security.auth.module.Krb5LoginModule required
+       useKeyTab=true
+       storeKey=true
+       keyTab="/etc/security/keytabs/kafka_server.keytab"
+       principal="kafka/kafka1.hostname.com@EXAMPLE.COM";
+   };
+   ```
 
-        3. Make sure the keytabs configured in the JAAS file are readable by the operating system user who is starting kafka broker.
-        4. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
-               
-               listeners=SASL_PLAINTEXT://host.name:port
-               security.inter.broker.protocol=SASL_PLAINTEXT
-               sasl.mechanism.inter.broker.protocol=GSSAPI
-               sasl.enabled.mechanisms=GSSAPI
+   `KafkaServer` section in the JAAS file tells the broker which principal to use and the location of the keytab where this principal is stored. It allows the broker to login using the keytab specified in this section.
 
-We must also configure the service name in server.properties, which should match the principal name of the kafka brokers. In the above example, principal is "kafka/kafka1.hostname.com@EXAMPLE.com", so: 
-               
-               sasl.kerberos.service.name=kafka
+2. Pass the JAAS and optionally the krb5 file locations as JVM parameters to each Kafka broker (see [here](https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html) for more details):
+
+   ```
+   -Djava.security.krb5.conf=/etc/kafka/krb5.conf
+   -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
+   ```
+
+3. Make sure the keytabs configured in the JAAS file are readable by the operating system user who is starting kafka broker.
+
+4. Configure SASL port and SASL mechanisms in server.properties as described here. For example:
+
+   ```
+   listeners=SASL_PLAINTEXT://host.name:port
+   security.inter.broker.protocol=SASL_PLAINTEXT
+   sasl.mechanism.inter.broker.protocol=GSSAPI
+   sasl.enabled.mechanisms=GSSAPI
+   ```
+
+   We must also configure the service name in server.properties, which should match the principal name of the kafka brokers. In the above example, principal is "kafka/kafka1.hostname.com@EXAMPLE.com", so:
+
+   ```
+   sasl.kerberos.service.name=kafka
+   ```
 
 #### Configuring Kafka Clients
 
 To configure SASL authentication on the clients: 
-        1. Clients (producers, consumers, connect workers, etc) will authenticate to the cluster with their own principal (usually with the same name as the user running the client), so obtain or create these principals as needed. Then configure the JAAS configuration property for each client. Different clients within a JVM may run as different users by specifying different principals. The property `sasl.jaas.config` in producer.properties or consumer.properties describes how clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client using a keytab (recommended for long-running processes): 
+1. Clients (producers, consumers, connect workers, etc) will authenticate to the cluster with their own principal (usually with the same name as the user running the client), so obtain or create these principals as needed. Then configure the JAAS configuration property for each client. Different clients within a JVM may run as different users by specifying different principals. The property `sasl.jaas.config` in producer.properties or consumer.properties describes how clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client using a keytab (recommended for long-running processes): 
                
                sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \
                    useKeyTab=true \
@@ -171,12 +196,12 @@ For command-line utilities like kafka-console-consumer or kafka-console-producer
                    useTicketCache=true;
 
 JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers as described here. Clients use the login section named `KafkaClient`. This option allows only one user for all client connections from a JVM.
-        2. Make sure the keytabs configured in the JAAS configuration are readable by the operating system user who is starting kafka client.
-        3. Optionally pass the krb5 file locations as JVM parameters to each client JVM (see [here](https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html) for more details): 
+2. Make sure the keytabs configured in the JAAS configuration are readable by the operating system user who is starting kafka client.
+3. Optionally pass the krb5 file locations as JVM parameters to each client JVM (see [here](https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html) for more details): 
                
                -Djava.security.krb5.conf=/etc/kafka/krb5.conf
 
-        4. Configure the following properties in producer.properties or consumer.properties: 
+4. Configure the following properties in producer.properties or consumer.properties: 
                
                security.protocol=SASL_PLAINTEXT (or SASL_SSL)
                sasl.mechanism=GSSAPI
@@ -189,7 +214,7 @@ SASL/PLAIN is a simple username/password authentication mechanism that is typica
 Under the default implementation of `principal.builder.class`, the username is used as the authenticated `Principal` for configuration of ACLs etc. 
 #### Configuring Kafka Brokers
 
-        1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
+1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
                
                KafkaServer {
                    org.apache.kafka.common.security.plain.PlainLoginModule required
@@ -200,11 +225,11 @@ Under the default implementation of `principal.builder.class`, the username is u
                };
 
 This configuration defines two users (_admin_ and _alice_). The properties `username` and `password` in the `KafkaServer` section are used by the broker to initiate connections to other brokers. In this example, _admin_ is the user for inter-broker communication. The set of properties `user__userName_` defines the passwords for all users that connect to the broker and the broker validates all client connections including those from other brokers using these properties.
-        2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
+2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
                
                -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
 
-        3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
+3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
                
                listeners=SASL_SSL://host.name:port
                security.inter.broker.protocol=SASL_SSL
@@ -214,7 +239,7 @@ This configuration defines two users (_admin_ and _alice_). The properties `user
 #### Configuring Kafka Clients
 
 To configure SASL authentication on the clients: 
-        1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the PLAIN mechanism: 
+1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the PLAIN mechanism: 
                
                sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
                    username="alice" \
@@ -224,16 +249,16 @@ The options `username` and `password` are used by clients to configure the user
 
 JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers as described here. Clients use the login section named `KafkaClient`. This option allows only one user for all client connections from a JVM.
 
-        2. Configure the following properties in producer.properties or consumer.properties: 
+2. Configure the following properties in producer.properties or consumer.properties: 
                
                security.protocol=SASL_SSL
                sasl.mechanism=PLAIN
 
 #### Use of SASL/PLAIN in production
 
-        * SASL/PLAIN should be used only with SSL as transport layer to ensure that clear passwords are not transmitted on the wire without encryption.
-        * The default implementation of SASL/PLAIN in Kafka specifies usernames and passwords in the JAAS configuration file as shown here. From Kafka version 2.0 onwards, you can avoid storing clear passwords on disk by configuring your own callback handlers that obtain username and password from an external source using the configuration options `sasl.server.callback.handler.class` and `sasl.client.callback.handler.class`.
-        * In production systems, external authentication servers may implement password authentication. From Kafka version 2.0 onwards, you can plug in your own callback handlers that use external authentication servers for password verification by configuring `sasl.server.callback.handler.class`.
+* SASL/PLAIN should be used only with SSL as transport layer to ensure that clear passwords are not transmitted on the wire without encryption.
+* The default implementation of SASL/PLAIN in Kafka specifies usernames and passwords in the JAAS configuration file as shown here. From Kafka version 2.0 onwards, you can avoid storing clear passwords on disk by configuring your own callback handlers that obtain username and password from an external source using the configuration options `sasl.server.callback.handler.class` and `sasl.client.callback.handler.class`.
+* In production systems, external authentication servers may implement password authentication. From Kafka version 2.0 onwards, you can plug in your own callback handlers that use external authentication servers for password verification by configuring `sasl.server.callback.handler.class`.
 ### Authentication using SASL/SCRAM
 
 Salted Challenge Response Authentication Mechanism (SCRAM) is a family of SASL mechanisms that addresses the security concerns with traditional mechanisms that perform username/password authentication like PLAIN and DIGEST-MD5. The mechanism is defined in [RFC 5802](https://tools.ietf.org/html/rfc5802). Kafka supports [SCRAM-SHA-256](https://tools.ietf.org/html/rfc7677) and SCRAM-SHA-512 which can be used with TLS to perform secure authentication. Under the default implementation of `principal.builder.class`, the username is used as the authenticated `Principal` for configuration of ACLs etc. The default SCRAM implementation in Kafka stores SCRAM credentials in the metadata log. Refer to Security Considerations for more details.
@@ -262,7 +287,7 @@ Credentials may be deleted for one or more SCRAM mechanisms using the _\--alter
 
 #### Configuring Kafka Brokers
 
-        1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
+1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
                
                KafkaServer {
                    org.apache.kafka.common.security.scram.ScramLoginModule required
@@ -271,11 +296,11 @@ Credentials may be deleted for one or more SCRAM mechanisms using the _\--alter
                };
 
 The properties `username` and `password` in the `KafkaServer` section are used by the broker to initiate connections to other brokers. In this example, _admin_ is the user for inter-broker communication.
-        2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
+2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
                
                -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
 
-        3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
+3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
                
                listeners=SASL_SSL://host.name:port
                security.inter.broker.protocol=SASL_SSL
@@ -285,7 +310,7 @@ The properties `username` and `password` in the `KafkaServer` section are used b
 #### Configuring Kafka Clients
 
 To configure SASL authentication on the clients: 
-        1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the SCRAM mechanisms: 
+1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the SCRAM mechanisms: 
                
                sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
                    username="alice" \
@@ -295,18 +320,18 @@ The options `username` and `password` are used by clients to configure the user
 
 JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers as described here. Clients use the login section named `KafkaClient`. This option allows only one user for all client connections from a JVM.
 
-        2. Configure the following properties in producer.properties or consumer.properties: 
+2. Configure the following properties in producer.properties or consumer.properties: 
                
                security.protocol=SASL_SSL
                sasl.mechanism=SCRAM-SHA-256 (or SCRAM-SHA-512)
 
 #### Security Considerations for SASL/SCRAM
 
-        * The default implementation of SASL/SCRAM in Kafka stores SCRAM credentials in the metadata log. This is suitable for production use in installations where KRaft controllers are secure and on a private network.
-        * Kafka supports only the strong hash functions SHA-256 and SHA-512 with a minimum iteration count of 4096. Strong hash functions combined with strong passwords and high iteration counts protect against brute force attacks if KRaft controllers security is compromised.
-        * SCRAM should be used only with TLS-encryption to prevent interception of SCRAM exchanges. This protects against dictionary or brute force attacks and against impersonation if KRaft controllers security is compromised.
-        * From Kafka version 2.0 onwards, the default SASL/SCRAM credential store may be overridden using custom callback handlers by configuring `sasl.server.callback.handler.class` in installations where KRaft controllers are not secure.
-        * For more details on security considerations, refer to [RFC 5802](https://tools.ietf.org/html/rfc5802#section-9).
+* The default implementation of SASL/SCRAM in Kafka stores SCRAM credentials in the metadata log. This is suitable for production use in installations where KRaft controllers are secure and on a private network.
+* Kafka supports only the strong hash functions SHA-256 and SHA-512 with a minimum iteration count of 4096. Strong hash functions combined with strong passwords and high iteration counts protect against brute force attacks if KRaft controllers security is compromised.
+* SCRAM should be used only with TLS-encryption to prevent interception of SCRAM exchanges. This protects against dictionary or brute force attacks and against impersonation if KRaft controllers security is compromised.
+* From Kafka version 2.0 onwards, the default SASL/SCRAM credential store may be overridden using custom callback handlers by configuring `sasl.server.callback.handler.class` in installations where KRaft controllers are not secure.
+* For more details on security considerations, refer to [RFC 5802](https://tools.ietf.org/html/rfc5802#section-9).
 ### Authentication using SASL/OAUTHBEARER
 
 The [OAuth 2 Authorization Framework](https://tools.ietf.org/html/rfc6749) "enables a third-party application to obtain limited access to an HTTP service, either on behalf of a resource owner by orchestrating an approval interaction between the resource owner and the HTTP service, or by allowing the third-party application to obtain access on its own behalf." The SASL OAUTHBEARER mechanism enables the use of the framework in a SASL (i.e. a non-HTTP) context; it is defined in [RFC 7628](https://tools.ietf.org/html/rfc7628). The default OAUTHBEARER implementation in Kafka creates and validates [Unsecured JSON Web Tokens](https://tools.ietf.org/html/rfc7515#appendix-A.5) and is only suitable for use in non-production Kafka installations. Refer to Security Considerations for more details. Recent versions of Apache Kafka have added production-ready OAUTHBEARER implementations that support interaction with an OAuth 2.0-standards compliant identity provider. Both modes are described in the following, noted where applicable.
@@ -316,7 +341,7 @@ Under the default implementation of `principal.builder.class`, the principalName
 
 The default implementation of SASL/OAUTHBEARER in Kafka creates and validates [Unsecured JSON Web Tokens](https://tools.ietf.org/html/rfc7515#appendix-A.5). While suitable only for non-production use, it does provide the flexibility to create arbitrary tokens in a DEV or TEST environment.
 
-        1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
+1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
                
                KafkaServer {
                    org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required
@@ -373,11 +398,11 @@ Set to a space-delimited list of scope values if you wish the `String/String Lis
 Set to a positive integer value if you wish to allow up to some number of positive milliseconds of clock skew (the default is 0).
 </td> </tr> </table>
 
-        2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
+2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
                
                -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
 
-        3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
+3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
                
                listeners=SASL_SSL://host.name:port (or SASL_PLAINTEXT if non-production)
                security.inter.broker.protocol=SASL_SSL (or SASL_PLAINTEXT if non-production)
@@ -386,17 +411,17 @@ Set to a positive integer value if you wish to allow up to some number of positi
 
 #### Configuring Production Kafka Brokers
 
-        1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
+1. Add a suitably modified JAAS file similar to the one below to each Kafka broker's config directory, let's call it kafka_server_jaas.conf for this example: 
                
                KafkaServer {
                    org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required ;
                };
 
-        2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
+2. Pass the JAAS config file location as JVM parameter to each Kafka broker: 
                
                -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
 
-        3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
+3. Configure SASL port and SASL mechanisms in server.properties as described here. For example: 
                
                listeners=SASL_SSL://host.name:port
                security.inter.broker.protocol=SASL_SSL
@@ -406,19 +431,19 @@ Set to a positive integer value if you wish to allow up to some number of positi
                listener.name.<listener name>.oauthbearer.sasl.oauthbearer.jwks.endpoint.url=https://example.com/oauth2/v1/keys
 
 The OAUTHBEARER broker configuration includes: 
-           * sasl.oauthbearer.clock.skew.seconds
-           * sasl.oauthbearer.expected.audience
-           * sasl.oauthbearer.expected.issuer
-           * sasl.oauthbearer.jwks.endpoint.refresh.ms
-           * sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms
-           * sasl.oauthbearer.jwks.endpoint.retry.backoff.ms
-           * sasl.oauthbearer.jwks.endpoint.url
-           * sasl.oauthbearer.scope.claim.name
-           * sasl.oauthbearer.sub.claim.name
+   * sasl.oauthbearer.clock.skew.seconds
+   * sasl.oauthbearer.expected.audience
+   * sasl.oauthbearer.expected.issuer
+   * sasl.oauthbearer.jwks.endpoint.refresh.ms
+   * sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms
+   * sasl.oauthbearer.jwks.endpoint.retry.backoff.ms
+   * sasl.oauthbearer.jwks.endpoint.url
+   * sasl.oauthbearer.scope.claim.name
+   * sasl.oauthbearer.sub.claim.name
 #### Configuring Non-production Kafka Clients
 
 To configure SASL authentication on the clients: 
-        1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the OAUTHBEARER mechanisms: 
+1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the OAUTHBEARER mechanisms: 
                
                sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
                    unsecuredLoginStringClaim_sub="alice";
@@ -503,22 +528,22 @@ Set to a custom claim name if you wish the name of the `String` or `String List`
 
 JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers as described here. Clients use the login section named `KafkaClient`. This option allows only one user for all client connections from a JVM.
 
-        2. Configure the following properties in producer.properties or consumer.properties: 
+2. Configure the following properties in producer.properties or consumer.properties: 
                
                security.protocol=SASL_SSL (or SASL_PLAINTEXT if non-production)
                sasl.mechanism=OAUTHBEARER
 
-        3. The default implementation of SASL/OAUTHBEARER depends on the jackson-databind library. Since it's an optional dependency, users have to configure it as a dependency via their build tool.
+3. The default implementation of SASL/OAUTHBEARER depends on the jackson-databind library. Since it's an optional dependency, users have to configure it as a dependency via their build tool.
 #### Configuring Production Kafka Clients
 
 To configure SASL authentication on the clients: 
-        1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the OAUTHBEARER mechanisms: 
+1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the OAUTHBEARER mechanisms: 
                
                sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required ;
 
 JAAS configuration for clients may alternatively be specified as a JVM parameter similar to brokers as described here. Clients use the login section named `KafkaClient`. This option allows only one user for all client connections from a JVM.
 
-        2. Configure the following properties in producer.properties or consumer.properties. For example, if using the OAuth `client_credentials` grant type to communicate with the OAuth identity provider, the configuration might look like this: 
+2. Configure the following properties in producer.properties or consumer.properties. For example, if using the OAuth `client_credentials` grant type to communicate with the OAuth identity provider, the configuration might look like this: 
                
                security.protocol=SASL_SSL
                sasl.mechanism=OAUTHBEARER
@@ -541,25 +566,25 @@ Or, if using the OAuth `urn:ietf:params:oauth:grant-type:jwt-bearer` grant type
                sasl.oauthbearer.token.endpoint.url=https://example.com/oauth2/v1/token
 
 The OAUTHBEARER client configuration includes: 
-           * sasl.oauthbearer.assertion.algorithm
-           * sasl.oauthbearer.assertion.claim.aud
-           * sasl.oauthbearer.assertion.claim.exp.seconds
-           * sasl.oauthbearer.assertion.claim.iss
-           * sasl.oauthbearer.assertion.claim.jti.include
-           * sasl.oauthbearer.assertion.claim.nbf.seconds
-           * sasl.oauthbearer.assertion.claim.sub
-           * sasl.oauthbearer.assertion.file
-           * sasl.oauthbearer.assertion.private.key.file
-           * sasl.oauthbearer.assertion.private.key.passphrase
-           * sasl.oauthbearer.assertion.template.file
-           * sasl.oauthbearer.client.credentials.client.id
-           * sasl.oauthbearer.client.credentials.client.secret
-           * sasl.oauthbearer.header.urlencode
-           * sasl.oauthbearer.jwt.retriever.class
-           * sasl.oauthbearer.jwt.validator.class
-           * sasl.oauthbearer.scope
-           * sasl.oauthbearer.token.endpoint.url
-        3. The default implementation of SASL/OAUTHBEARER depends on the jackson-databind library. Since it's an optional dependency, users have to configure it as a dependency via their build tool.
+   * sasl.oauthbearer.assertion.algorithm
+   * sasl.oauthbearer.assertion.claim.aud
+   * sasl.oauthbearer.assertion.claim.exp.seconds
+   * sasl.oauthbearer.assertion.claim.iss
+   * sasl.oauthbearer.assertion.claim.jti.include
+   * sasl.oauthbearer.assertion.claim.nbf.seconds
+   * sasl.oauthbearer.assertion.claim.sub
+   * sasl.oauthbearer.assertion.file
+   * sasl.oauthbearer.assertion.private.key.file
+   * sasl.oauthbearer.assertion.private.key.passphrase
+   * sasl.oauthbearer.assertion.template.file
+   * sasl.oauthbearer.client.credentials.client.id
+   * sasl.oauthbearer.client.credentials.client.secret
+   * sasl.oauthbearer.header.urlencode
+   * sasl.oauthbearer.jwt.retriever.class
+   * sasl.oauthbearer.jwt.validator.class
+   * sasl.oauthbearer.scope
+   * sasl.oauthbearer.token.endpoint.url
+3. The default implementation of SASL/OAUTHBEARER depends on the jackson-databind library. Since it's an optional dependency, users have to configure it as a dependency via their build tool.
 #### Token Refresh for SASL/OAUTHBEARER
 
 Kafka periodically refreshes any token before it expires so that the client can continue to make connections to brokers. The parameters that impact how the refresh algorithm operates are specified as part of the producer/consumer/broker configuration and are as follows. See the documentation for these properties elsewhere for details. The default values are usually reasonable, in which case these configuration parameters would not need to be explicitly set.   
@@ -596,13 +621,13 @@ Production use cases will require writing an implementation of `org.apache.kafka
 Production use cases will also require writing an implementation of `org.apache.kafka.common.security.auth.AuthenticateCallbackHandler` that can handle an instance of `org.apache.kafka.common.security.oauthbearer.OAuthBearerValidatorCallback` and declaring it via the `listener.name.sasl_ssl.oauthbearer.sasl.server.callback.handler.class` broker configuration option. 
 #### Security Considerations for SASL/OAUTHBEARER
 
-        * The default implementation of SASL/OAUTHBEARER in Kafka creates and validates [Unsecured JSON Web Tokens](https://tools.ietf.org/html/rfc7515#appendix-A.5). This is suitable only for non-production use.
-        * OAUTHBEARER should be used in production environments only with TLS-encryption to prevent interception of tokens.
-        * The default unsecured SASL/OAUTHBEARER implementation may be overridden (and must be overridden in production environments) using custom login and SASL Server callback handlers as described above.
-        * For more details on OAuth 2 security considerations in general, refer to [RFC 6749, Section 10](https://tools.ietf.org/html/rfc6749#section-10).
+* The default implementation of SASL/OAUTHBEARER in Kafka creates and validates [Unsecured JSON Web Tokens](https://tools.ietf.org/html/rfc7515#appendix-A.5). This is suitable only for non-production use.
+* OAUTHBEARER should be used in production environments only with TLS-encryption to prevent interception of tokens.
+* The default unsecured SASL/OAUTHBEARER implementation may be overridden (and must be overridden in production environments) using custom login and SASL Server callback handlers as described above.
+* For more details on OAuth 2 security considerations in general, refer to [RFC 6749, Section 10](https://tools.ietf.org/html/rfc6749#section-10).
 ### Enabling multiple SASL mechanisms in a broker
 
-     1. Specify configuration for the login modules of all enabled mechanisms in the `KafkaServer` section of the JAAS config file. For example: 
+1. Specify configuration for the login modules of all enabled mechanisms in the `KafkaServer` section of the JAAS config file. For example: 
             
             KafkaServer {
                 com.sun.security.auth.module.Krb5LoginModule required
@@ -618,24 +643,24 @@ Production use cases will also require writing an implementation of `org.apache.
                 user_alice="alice-secret";
             };
 
-     2. Enable the SASL mechanisms in server.properties: 
+2. Enable the SASL mechanisms in server.properties: 
             
             sasl.enabled.mechanisms=GSSAPI,PLAIN,SCRAM-SHA-256,SCRAM-SHA-512,OAUTHBEARER
 
-     3. Specify the SASL security protocol and mechanism for inter-broker communication in server.properties if required: 
+3. Specify the SASL security protocol and mechanism for inter-broker communication in server.properties if required: 
             
             security.inter.broker.protocol=SASL_PLAINTEXT (or SASL_SSL)
             sasl.mechanism.inter.broker.protocol=GSSAPI (or one of the other enabled mechanisms)
 
-     4. Follow the mechanism-specific steps in GSSAPI (Kerberos), PLAIN, SCRAM, and non-production/production OAUTHBEARER to configure SASL for the enabled mechanisms.
+4. Follow the mechanism-specific steps in GSSAPI (Kerberos), PLAIN, SCRAM, and non-production/production OAUTHBEARER to configure SASL for the enabled mechanisms.
 ### Modifying SASL mechanism in a Running Cluster
 
 SASL mechanism can be modified in a running cluster using the following sequence:
 
-     1. Enable new SASL mechanism by adding the mechanism to `sasl.enabled.mechanisms` in server.properties for each broker. Update JAAS config file to include both mechanisms as described here. Incrementally bounce the cluster nodes.
-     2. Restart clients using the new mechanism.
-     3. To change the mechanism of inter-broker communication (if this is required), set `sasl.mechanism.inter.broker.protocol` in server.properties to the new mechanism and incrementally bounce the cluster again.
-     4. To remove old mechanism (if this is required), remove the old mechanism from `sasl.enabled.mechanisms` in server.properties and remove the entries for the old mechanism from JAAS config file. Incrementally bounce the cluster again.
+1. Enable new SASL mechanism by adding the mechanism to `sasl.enabled.mechanisms` in server.properties for each broker. Update JAAS config file to include both mechanisms as described here. Incrementally bounce the cluster nodes.
+2. Restart clients using the new mechanism.
+3. To change the mechanism of inter-broker communication (if this is required), set `sasl.mechanism.inter.broker.protocol` in server.properties to the new mechanism and incrementally bounce the cluster again.
+4. To remove old mechanism (if this is required), remove the old mechanism from `sasl.enabled.mechanisms` in server.properties and remove the entries for the old mechanism from JAAS config file. Incrementally bounce the cluster again.
 ### Authentication using Delegation Tokens
 
 Delegation token based authentication is a lightweight authentication mechanism to complement existing SASL/SSL methods. Delegation tokens are shared secrets between kafka brokers and clients. Delegation tokens will help processing frameworks to distribute the workload to available workers in a secure environment without the added cost of distributing Kerberos TGT/keytabs or keystores when 2-way SSL is used. See [KIP-48](https://cwiki.apache.org/confluence/x/tfmnAw) for more details.
@@ -644,9 +669,9 @@ Under the default implementation of `principal.builder.class`, the owner of dele
 
 Typical steps for delegation token usage are:
 
-     1. User authenticates with the Kafka cluster via SASL or SSL, and obtains a delegation token. This can be done using Admin APIs or using `kafka-delegation-tokens.sh` script.
-     2. User securely passes the delegation token to Kafka clients for authenticating with the Kafka cluster.
-     3. Token owner/renewer can renew/expire the delegation tokens.
+1. User authenticates with the Kafka cluster via SASL or SSL, and obtains a delegation token. This can be done using Admin APIs or using `kafka-delegation-tokens.sh` script.
+2. User securely passes the delegation token to Kafka clients for authenticating with the Kafka cluster.
+3. Token owner/renewer can renew/expire the delegation tokens.
 #### Token Management
 
 A secret is used to generate and verify delegation tokens. This is supplied using config option `delegation.token.secret.key`. The same secret key must be configured across all the brokers. The controllers must also be configured with the secret using the same config option. If the secret is not set or set to empty string, delegation token authentication and API operations will fail.
@@ -687,7 +712,7 @@ Delegation token authentication piggybacks on the current SASL/SCRAM authenticat
 
 Configuring Kafka Clients:
 
-        1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the token authentication: 
+1. Configure the JAAS configuration property for each client in producer.properties or consumer.properties. The login module describes how the clients like producer and consumer can connect to the Kafka Broker. The following is an example configuration for a client for the token authentication: 
                
                sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
                    username="tokenID123" \
@@ -702,9 +727,9 @@ JAAS configuration for clients may alternatively be specified as a JVM parameter
 
 We require a re-deployment when the secret needs to be rotated. During this process, already connected clients will continue to work. But any new connection requests and renew/expire requests with old tokens can fail. Steps are given below.
 
-        1. Expire all existing tokens.
-        2. Rotate the secret by rolling upgrade, and
-        3. Generate new tokens
+1. Expire all existing tokens.
+2. Rotate the secret by rolling upgrade, and
+3. Generate new tokens
 
 We intend to automate this in a future Kafka release.
 
diff --git a/docs/security/encryption-and-authentication-using-ssl.md b/docs/security/encryption-and-authentication-using-ssl.md
index eb1c5e592c..11dd9fad03 100644
--- a/docs/security/encryption-and-authentication-using-ssl.md
+++ b/docs/security/encryption-and-authentication-using-ssl.md
@@ -35,8 +35,8 @@ The first step of deploying one or more brokers with SSL support is to generate
          $ keytool -keystore {keystorefile} -alias localhost -validity {validity} -genkey -keyalg RSA -storetype pkcs12
 
 You need to specify two parameters in the above command: 
-     1. keystorefile: the keystore file that stores the keys (and later the certificate) for this broker. The keystore file contains the private and public keys of this broker, therefore it needs to be kept safe. Ideally this step is run on the Kafka broker that the key will be used on, as this key should never be transmitted/leave the server that it is intended for.
-     2. validity: the valid time of the key in days. Please note that this differs from the validity period for the certificate, which will be determined in Signing the certificate. You can use the same key to request multiple certificates: if your key has a validity of 10 years, but your CA will only sign certificates that are valid for one year, you can use the same key with 10 certificates over time.
+1. keystorefile: the keystore file that stores the keys (and later the certificate) for this broker. The keystore file contains the private and public keys of this broker, therefore it needs to be kept safe. Ideally this step is run on the Kafka broker that the key will be used on, as this key should never be transmitted/leave the server that it is intended for.
+2. validity: the valid time of the key in days. Please note that this differs from the validity period for the certificate, which will be determined in Signing the certificate. You can use the same key to request multiple certificates: if your key has a validity of 10 years, but your CA will only sign certificates that are valid for one year, you can use the same key with 10 certificates over time.
   
 To obtain a certificate that can be used with the private key that was just created a certificate signing request needs to be created. This signing request, when signed by a trusted CA results in the actual certificate which can then be installed in the keystore and used for authentication purposes.  
 To generate certificate signing requests run the following command for all server keystores created so far. 
@@ -61,8 +61,8 @@ Normally there is no good reason to disable hostname verification apart from bei
 Getting hostname verification right is not that hard when done at the right time, but gets much harder once the cluster is up and running - do yourself a favor and do it now! 
 
 If host name verification is enabled, clients will verify the server's fully qualified domain name (FQDN) or ip address against one of the following two fields: 
-     1. Common Name (CN)
-     2. [Subject Alternative Name (SAN)](https://tools.ietf.org/html/rfc5280#section-4.2.1.6)
+1. Common Name (CN)
+2. [Subject Alternative Name (SAN)](https://tools.ietf.org/html/rfc5280#section-4.2.1.6)
   
 While Kafka checks both fields, usage of the common name field for hostname verification has been [deprecated](https://tools.ietf.org/html/rfc2818#section-3.1) since 2000 and should be avoided if possible. In addition the SAN field is much more flexible, allowing for multiple DNS and IP entries to be declared in a certificate.  
 Another advantage is that if the SAN field is used for hostname verification the common name can be set to a more meaningful value for authorization purposes. Since we need the SAN field to be contained in the signed certificate, it will be specified when generating the signing request. It can also be specified when generating the keypair, but this will not automatically be copied into the signing request.  
@@ -195,10 +195,10 @@ Finally, you need to import both the certificate of the CA and the signed certif
          $ keytool -keystore {keystore} -alias localhost -import -file cert-signed
 
 The definitions of the parameters are the following: 
-     1. keystore: the location of the keystore
-     2. CA certificate: the certificate of the CA
-     3. certificate signing request: the csr created with the server key
-     4. server certificate: the file to write the signed certificate of the server to
+1. keystore: the location of the keystore
+2. CA certificate: the certificate of the CA
+3. certificate signing request: the csr created with the server key
+4. server certificate: the file to write the signed certificate of the server to
 This will leave you with one truststore called _truststore.jks_ \- this can be the same for all clients and brokers and does not contain any sensitive information, so there is no need to secure this.  
 Additionally you will have one _server.keystore.jks_ file per node which contains that nodes keys, certificate and your CAs certificate, please refer to Configuring Kafka Brokers and Configuring Kafka Clients for information on how to use these files. 
 
@@ -213,15 +213,15 @@ Store password configs `ssl.keystore.password` and `ssl.truststore.password` are
 ### Common Pitfalls in Production
 
 The above paragraphs show the process to create your own CA and use it to sign certificates for your cluster. While very useful for sandbox, dev, test, and similar systems, this is usually not the correct process to create certificates for a production cluster in a corporate environment. Enterprises will normally operate their own CA and users can send in CSRs to be signed with this CA, which has the benefit of users not being responsible to keep the CA secure as well as a central authority that everybody can trust. However it also takes away a lot of control over the process of signing certificates from the user. Quite often the persons operating corporate CAs will apply tight restrictions on certificates that can cause issues when trying to use these certificates with Kafka. 
-     1. **[Extended Key Usage](https://tools.ietf.org/html/rfc5280#section-4.2.1.12)**  
+1. **[Extended Key Usage](https://tools.ietf.org/html/rfc5280#section-4.2.1.12)**  
 Certificates may contain an extension field that controls the purpose for which the certificate can be used. If this field is empty, there are no restrictions on the usage, but if any usage is specified in here, valid SSL implementations have to enforce these usages.  
 Relevant usages for Kafka are: 
-        * Client authentication
-        * Server authentication
+* Client authentication
+* Server authentication
 Kafka brokers need both these usages to be allowed, as for intra-cluster communication every broker will behave as both the client and the server towards other brokers. It is not uncommon for corporate CAs to have a signing profile for webservers and use this for Kafka as well, which will only contain the _serverAuth_ usage value and cause the SSL handshake to fail. 
-     2. **Intermediate Certificates**  
+2. **Intermediate Certificates**  
 Corporate Root CAs are often kept offline for security reasons. To enable day-to-day usage, so called intermediate CAs are created, which are then used to sign the final certificates. When importing a certificate into the keystore that was signed by an intermediate CA it is necessary to provide the entire chain of trust up to the root CA. This can be done by simply _cat_ ing the certificate files into one combined certificate file and then importing this with keytool. 
-     3. **Failure to copy extension fields**  
+3. **Failure to copy extension fields**  
 CA operators are often hesitant to copy and requested extension fields from CSRs and prefer to specify these themselves as this makes it harder for a malicious party to obtain certificates with potentially misleading or fraudulent values. It is advisable to double check signed certificates, whether these contain all requested SAN fields to enable proper hostname verification. The following command can be used to print certificate details to the console, which should be compared with what was originally requested: 
             
             $ openssl x509 -in certificate.crt -text -noout
@@ -241,12 +241,12 @@ Following SSL configs are needed on the broker side
          ssl.truststore.password=test1234
 
 Note: ssl.truststore.password is technically optional but highly recommended. If a password is not set access to the truststore is still available, but integrity checking is disabled. Optional settings that are worth considering: 
-     1. ssl.client.auth=none ("required" => client authentication is required, "requested" => client authentication is requested and client without certs can still connect. The usage of "requested" is discouraged as it provides a false sense of security and misconfigured clients will still connect successfully.)
-     2. ssl.cipher.suites (Optional). A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. (Default is an empty list)
-     3. ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1 (list out the SSL protocols that you are going to accept from clients. Do note that SSL is deprecated in favor of TLS and using SSL in production is not recommended)
-     4. ssl.keystore.type=JKS
-     5. ssl.truststore.type=JKS
-     6. ssl.secure.random.implementation=SHA1PRNG
+1. ssl.client.auth=none ("required" => client authentication is required, "requested" => client authentication is requested and client without certs can still connect. The usage of "requested" is discouraged as it provides a false sense of security and misconfigured clients will still connect successfully.)
+2. ssl.cipher.suites (Optional). A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. (Default is an empty list)
+3. ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1 (list out the SSL protocols that you are going to accept from clients. Do note that SSL is deprecated in favor of TLS and using SSL in production is not recommended)
+4. ssl.keystore.type=JKS
+5. ssl.truststore.type=JKS
+6. ssl.secure.random.implementation=SHA1PRNG
 If you want to enable SSL for inter-broker communication, add the following to the server.properties file (it defaults to PLAINTEXT) 
     
     security.inter.broker.protocol=SSL
@@ -289,11 +289,11 @@ Note: ssl.truststore.password is technically optional but highly recommended. If
          ssl.key.password=test1234
 
 Other configuration settings that may also be needed depending on our requirements and the broker configuration: 
-     1. ssl.provider (Optional). The name of the security provider used for SSL connections. Default value is the default security provider of the JVM.
-     2. ssl.cipher.suites (Optional). A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol.
-     3. ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1. It should list at least one of the protocols configured on the broker side
-     4. ssl.truststore.type=JKS
-     5. ssl.keystore.type=JKS
+1. ssl.provider (Optional). The name of the security provider used for SSL connections. Default value is the default security provider of the JVM.
+2. ssl.cipher.suites (Optional). A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol.
+3. ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1. It should list at least one of the protocols configured on the broker side
+4. ssl.truststore.type=JKS
+5. ssl.keystore.type=JKS
   
 Examples using console-producer and console-consumer: 
     
diff --git a/docs/security/security-overview.md b/docs/security/security-overview.md
index d976c25295..f13f863531 100644
--- a/docs/security/security-overview.md
+++ b/docs/security/security-overview.md
@@ -26,15 +26,18 @@ type: docs
 -->
 
 
-The following security measures are currently supported: 
-
-  1. Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms: 
-     * SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0
-     * SASL/PLAIN - starting at version 0.10.0.0
-     * SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.10.2.0
-     * SASL/OAUTHBEARER - starting at version 2.0
-  2. Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.)
-  3. Authorization of read / write operations by clients
-  4. Authorization is pluggable and integration with external authorization services is supported
+The following security measures are currently supported:
+
+1. Authentication of connections to brokers from clients (producers and consumers), other brokers and tools, using either SSL or SASL. Kafka supports the following SASL mechanisms:
+   * SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0
+   * SASL/PLAIN - starting at version 0.10.0.0
+   * SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.10.2.0
+   * SASL/OAUTHBEARER - starting at version 2.0
+
+2. Encryption of data transferred between brokers and clients, between brokers, or between brokers and tools using SSL (Note that there is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation.)
+
+3. Authorization of read / write operations by clients
+
+4. Authorization is pluggable and integration with external authorization services is supported
 
 It's worth noting that security is optional - non-secured clusters are supported, as well as a mix of authenticated, unauthenticated, encrypted and non-encrypted clients. The guides below explain how to configure and use the security features in both clients and brokers. 
diff --git a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java
index b07669ca72..9eb6d1cfd3 100644
--- a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java
+++ b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinator.java
@@ -58,6 +58,7 @@ import org.apache.kafka.image.MetadataDelta;
 import org.apache.kafka.image.MetadataImage;
 import org.apache.kafka.server.authorizer.AuthorizableRequestContext;
 
+import java.time.Duration;
 import java.util.List;
 import java.util.Optional;
 import java.util.OptionalInt;
@@ -385,6 +386,7 @@ public interface GroupCoordinator {
      * @param coordinatorEpoch  The epoch of the transaction coordinator.
      * @param result            The transaction result.
      * @param transactionVersion The transaction version (1 = TV1, 2 = TV2, etc.).
+     * @param timeout           The operation timeout.
      *
      * @return A future yielding the result.
      */
@@ -394,7 +396,8 @@ public interface GroupCoordinator {
         short producerEpoch,
         int coordinatorEpoch,
         TransactionResult result,
-        short transactionVersion
+        short transactionVersion,
+        Duration timeout
     );
 
     /**
diff --git a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java
index c3a8e0e479..f98738ab80 100644
--- a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java
+++ b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java
@@ -269,7 +269,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
                     .withPartitionWriter(writer)
                     .withLoader(loader)
                     .withCoordinatorShardBuilderSupplier(supplier)
-                    .withWriteTimeout(Duration.ofMillis(config.offsetCommitTimeoutMs()))
+                    .withDefaultWriteTimeOut(Duration.ofMillis(config.offsetCommitTimeoutMs()))
                     .withCoordinatorRuntimeMetrics(coordinatorRuntimeMetrics)
                     .withCoordinatorMetrics(groupCoordinatorMetrics)
                     .withSerializer(new GroupCoordinatorRecordSerde())
@@ -494,6 +494,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "consumer-group-heartbeat",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.consumerGroupHeartbeat(context, request)
         ).exceptionally(exception -> handleOperationException(
             "consumer-group-heartbeat",
@@ -621,6 +622,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "streams-group-heartbeat",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.streamsGroupHeartbeat(context, request)
         ).exceptionally(exception -> handleOperationException(
             "streams-group-heartbeat",
@@ -686,6 +688,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "share-group-heartbeat",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.shareGroupHeartbeat(context, request)
         ).thenCompose(result -> {
             if (result.getValue().isPresent()) {
@@ -865,6 +868,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "uninitialize-share-group-state",
             topicPartitionFor(groupId),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.uninitializeShareGroupState(groupId, topicPartitionMap)
         ).thenApply(__ -> new ShareGroupHeartbeatResponseData()
             .setErrorCode(error.code())
@@ -886,6 +890,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "initialize-share-group-state",
             topicPartitionFor(groupId),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.initializeShareGroupState(groupId, topicPartitionMap)
         ).handle((__, exp) -> {
             if (exp == null) {
@@ -933,6 +938,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         runtime.scheduleWriteOperation(
             "classic-group-join",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.classicGroupJoin(context, request, responseFuture)
         ).exceptionally(exception -> {
             if (!responseFuture.isDone()) {
@@ -976,6 +982,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         runtime.scheduleWriteOperation(
             "classic-group-sync",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.classicGroupSync(context, request, responseFuture)
         ).exceptionally(exception -> {
             if (!responseFuture.isDone()) {
@@ -1016,6 +1023,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "classic-group-heartbeat",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.classicGroupHeartbeat(context, request)
         ).exceptionally(exception -> handleOperationException(
             "classic-group-heartbeat",
@@ -1058,6 +1066,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "classic-group-leave",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.classicGroupLeave(context, request)
         ).exceptionally(exception -> handleOperationException(
             "classic-group-leave",
@@ -1308,6 +1317,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "share-group-offsets-alter",
             topicPartitionFor(groupId),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.alterShareGroupOffsets(groupId, request)
         ).thenCompose(result ->
             persisterInitialize(result.getValue(), result.getKey())
@@ -1490,6 +1500,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "delete-groups",
             topicPartition,
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.deleteGroups(context, groupIds)
         ).exceptionally(exception -> handleOperationException(
             "delete-groups",
@@ -1508,6 +1519,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "delete-share-groups",
             topicPartition,
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.sharePartitionDeleteRequests(groupList)
         ).thenCompose(
             this::performShareGroupsDeletion
@@ -1638,6 +1650,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
             return runtime.scheduleWriteOperation(
                 name,
                 topicPartitionFor(request.groupId()),
+                Duration.ofMillis(config.offsetCommitTimeoutMs()),
                 coordinator -> new CoordinatorResult<>(
                     List.of(),
                     coordinator.fetchOffsets(request, Long.MAX_VALUE)
@@ -1940,6 +1953,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "initiate-delete-share-group-offsets",
             topicPartitionFor(groupId),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.initiateDeleteShareGroupOffsets(groupId, requestData)
         ).thenCompose(resultHolder -> deleteShareGroupOffsetsState(groupId, resultHolder)
         ).exceptionally(exception -> handleOperationException(
@@ -2044,6 +2058,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "complete-delete-share-group-offsets",
             topicPartitionFor(groupId),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.completeDeleteShareGroupOffsets(groupId, successTopics, errorTopicResponses)
         ).exceptionally(exception -> handleOperationException(
             "complete-delete-share-group-offsets",
@@ -2081,6 +2096,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "commit-offset",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.commitOffset(context, request)
         ).exceptionally(exception -> handleOperationException(
             "commit-offset",
@@ -2120,6 +2136,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
             request.transactionalId(),
             request.producerId(),
             request.producerEpoch(),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.commitTransactionalOffset(context, request),
             context.requestVersion()
         ).exceptionally(exception -> handleOperationException(
@@ -2155,6 +2172,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
         return runtime.scheduleWriteOperation(
             "delete-offsets",
             topicPartitionFor(request.groupId()),
+            Duration.ofMillis(config.offsetCommitTimeoutMs()),
             coordinator -> coordinator.deleteOffsets(context, request)
         ).exceptionally(exception -> handleOperationException(
             "delete-offsets",
@@ -2166,7 +2184,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
     }
 
     /**
-     * See {@link GroupCoordinator#completeTransaction(TopicPartition, long, short, int, TransactionResult, short)}.
+     * See {@link GroupCoordinator#completeTransaction(TopicPartition, long, short, int, TransactionResult, short, Duration)}.
      */
     @Override
     public CompletableFuture<Void> completeTransaction(
@@ -2175,7 +2193,8 @@ public class GroupCoordinatorService implements GroupCoordinator {
         short producerEpoch,
         int coordinatorEpoch,
         TransactionResult result,
-        short transactionVersion
+        short transactionVersion,
+        Duration timeout
     ) {
         if (!isActive.get()) {
             return FutureUtils.failedFuture(Errors.COORDINATOR_NOT_AVAILABLE.exception());
@@ -2194,7 +2213,8 @@ public class GroupCoordinatorService implements GroupCoordinator {
             producerEpoch,
             coordinatorEpoch,
             result,
-            transactionVersion
+            transactionVersion,
+            timeout
         );
     }
 
@@ -2277,6 +2297,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
                 FutureUtils.mapExceptionally(
                     runtime.scheduleWriteAllOperation(
                         "on-topics-deleted",
+                        Duration.ofMillis(config.offsetCommitTimeoutMs()),
                         coordinator -> coordinator.onTopicsDeleted(deletedTopics)
                     ),
                     exception -> {
@@ -2294,6 +2315,7 @@ public class GroupCoordinatorService implements GroupCoordinator {
                 FutureUtils.mapExceptionally(
                     runtime.scheduleWriteAllOperation(
                         "maybe-cleanup-share-group-state",
+                        Duration.ofMillis(config.offsetCommitTimeoutMs()),
                         coordinator -> coordinator.maybeCleanupShareGroupState(deletedTopicIds)
                     ),
                     exception -> {
diff --git a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilder.java b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilder.java
index cf1d3ddbec..badedefdee 100644
--- a/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilder.java
+++ b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilder.java
@@ -197,7 +197,10 @@ public class CurrentAssignmentBuilder {
                         member.assignedPartitions()
                     );
                 } else if (hasSubscriptionChanged) {
-                    return updateCurrentAssignment(member.assignedPartitions());
+                    return updateCurrentAssignment(
+                        member.memberEpoch(),
+                        member.assignedPartitions()
+                    );
                 } else {
                     return member;
                 }
@@ -214,7 +217,10 @@ public class CurrentAssignmentBuilder {
                 // owns any of the revoked partitions. If it does, we cannot progress.
                 if (ownsRevokedPartitions(member.partitionsPendingRevocation())) {
                     if (hasSubscriptionChanged) {
-                        return updateCurrentAssignment(member.assignedPartitions());
+                        return updateCurrentAssignment(
+                            member.memberEpoch(),
+                            member.assignedPartitions()
+                        );
                     } else {
                         return member;
                     }
@@ -284,10 +290,12 @@ public class CurrentAssignmentBuilder {
      * Updates the current assignment, removing any partitions that are not part of the subscribed topics.
      * This method is a lot faster than running the full reconciliation logic in computeNextAssignment.
      *
+     * @param memberEpoch               The epoch of the member to use.
      * @param memberAssignedPartitions  The assigned partitions of the member to use.
      * @return A new ConsumerGroupMember.
      */
     private ConsumerGroupMember updateCurrentAssignment(
+        int memberEpoch,
         Map<Uuid, Set<Integer>> memberAssignedPartitions
     ) {
         Set<Uuid> subscribedTopicIds = subscribedTopicIds();
@@ -329,6 +337,7 @@ public class CurrentAssignmentBuilder {
         if (!newPartitionsPendingRevocation.isEmpty() && ownsRevokedPartitions(newPartitionsPendingRevocation)) {
             return new ConsumerGroupMember.Builder(member)
                 .setState(MemberState.UNREVOKED_PARTITIONS)
+                .updateMemberEpoch(memberEpoch)
                 .setAssignedPartitions(newAssignedPartitions)
                 .setPartitionsPendingRevocation(newPartitionsPendingRevocation)
                 .build();
@@ -340,6 +349,7 @@ public class CurrentAssignmentBuilder {
             // reconciliation logic should handle the case where the member has revoked all its
             // partitions pending revocation.
             return new ConsumerGroupMember.Builder(member)
+                .updateMemberEpoch(memberEpoch)
                 .setAssignedPartitions(newAssignedPartitions)
                 .build();
         }
diff --git a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java
index 11b0a4dde0..75f9d68d62 100644
--- a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java
+++ b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupCoordinatorServiceTest.java
@@ -127,6 +127,7 @@ import org.mockito.ArgumentCaptor;
 import org.mockito.ArgumentMatchers;
 
 import java.net.InetAddress;
+import java.time.Duration;
 import java.util.Arrays;
 import java.util.HashSet;
 import java.util.List;
@@ -244,6 +245,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("consumer-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             new ConsumerGroupHeartbeatResponseData()
@@ -281,6 +283,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("consumer-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(exception));
 
@@ -499,6 +502,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("streams-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             new StreamsGroupHeartbeatResult(
@@ -558,6 +562,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("streams-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(exception));
 
@@ -1047,6 +1052,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-join"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             new JoinGroupResponseData()
@@ -1076,6 +1082,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-join"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(new IllegalStateException()));
 
@@ -1200,6 +1207,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-sync"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             new SyncGroupResponseData()
@@ -1228,6 +1236,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-sync"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(new IllegalStateException()));
 
@@ -1308,6 +1317,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             new HeartbeatResponseData()
@@ -1336,6 +1346,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(
             new CoordinatorLoadInProgressException(null)
@@ -1364,6 +1375,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(
             new RebalanceInProgressException()
@@ -1733,6 +1745,7 @@ public class GroupCoordinatorServiceTest {
             when(runtime.scheduleWriteOperation(
                 ArgumentMatchers.eq(fetchAllOffsets ? "fetch-all-offsets" : "fetch-offsets"),
                 ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+                ArgumentMatchers.eq(Duration.ofMillis(5000)),
                 ArgumentMatchers.any()
             )).thenReturn(CompletableFuture.completedFuture(response));
         } else {
@@ -1836,6 +1849,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq(fetchAllOffsets ? "fetch-all-offsets" : "fetch-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(new CompletionException(error.exception())));
 
@@ -1867,6 +1881,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-leave"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             new LeaveGroupResponseData()
@@ -1903,6 +1918,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("classic-group-leave"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(
             new UnknownMemberIdException()
@@ -2265,6 +2281,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(response));
 
@@ -2304,6 +2321,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(response));
 
@@ -2348,6 +2366,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(exception));
 
@@ -2423,12 +2442,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-share-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(Map.of()));
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 2)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(resultCollection1));
 
@@ -2436,12 +2457,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(resultCollectionFuture);
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 1)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(Errors.COORDINATOR_LOAD_IN_PROGRESS.exception()));
 
@@ -2500,6 +2523,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-share-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             Map.of("share-group-id-1", Map.entry(createDeleteShareRequest("share-group-id-1", shareGroupTopicId, List.of(0, 1)), Errors.NONE))
@@ -2522,6 +2546,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 1)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(resultCollection1));
 
@@ -2529,6 +2554,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(resultCollection2));
 
@@ -2579,6 +2605,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-share-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             Map.of("share-group-id-1", Map.entry(createDeleteShareRequest("share-group-id-1", shareGroupTopicId, List.of(0, 1)), Errors.NONE))
@@ -2614,6 +2641,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 1)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(resultCollection1));
 
@@ -2621,6 +2649,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 2)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(resultCollection2));
 
@@ -2662,6 +2691,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-share-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.failedFuture(
             Errors.COORDINATOR_NOT_AVAILABLE.exception()
@@ -2671,6 +2701,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 1)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(resultCollection1));
 
@@ -2712,6 +2743,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-share-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             Map.of("share-group-id-1", Map.entry(EMPTY_PARAMS, Errors.forException(new GroupNotEmptyException("bad stuff"))))
@@ -2729,6 +2761,7 @@ public class GroupCoordinatorServiceTest {
         verify(runtime, times(0)).scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
     }
@@ -2763,6 +2796,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-share-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             Map.of("share-group-id-1", Map.entry(createDeleteShareRequest("share-group-id-1", shareGroupTopicId, List.of(0, 1)), Errors.NONE))
@@ -2772,6 +2806,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 1)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.failedFuture(Errors.CLUSTER_AUTHORIZATION_FAILED.exception()));
 
@@ -2812,12 +2847,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-share-groups"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(Map.of()));
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("delete-groups"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(exception));
 
@@ -2973,6 +3010,7 @@ public class GroupCoordinatorServiceTest {
             ArgumentMatchers.eq("transactional-id"),
             ArgumentMatchers.eq(10L),
             ArgumentMatchers.eq((short) 5),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any(),
             ArgumentMatchers.eq((int) txnOffsetCommitVersion)
         )).thenReturn(CompletableFuture.completedFuture(response));
@@ -3027,6 +3065,7 @@ public class GroupCoordinatorServiceTest {
             ArgumentMatchers.eq("transactional-id"),
             ArgumentMatchers.eq(10L),
             ArgumentMatchers.eq((short) 5),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any(),
             ArgumentMatchers.eq((int) ApiKeys.TXN_OFFSET_COMMIT.latestVersion())
         )).thenReturn(FutureUtils.failedFuture(new CompletionException(error.exception())));
@@ -3055,7 +3094,8 @@ public class GroupCoordinatorServiceTest {
             ArgumentMatchers.eq((short) 5),
             ArgumentMatchers.eq(10),
             ArgumentMatchers.eq(TransactionResult.COMMIT),
-            ArgumentMatchers.eq(TransactionVersion.TV_1.featureLevel())
+            ArgumentMatchers.eq(TransactionVersion.TV_1.featureLevel()),
+            ArgumentMatchers.eq(Duration.ofMillis(100))
         )).thenReturn(CompletableFuture.completedFuture(null));
 
         CompletableFuture<Void> future = service.completeTransaction(
@@ -3064,7 +3104,8 @@ public class GroupCoordinatorServiceTest {
             (short) 5,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            Duration.ofMillis(100)
         );
 
         assertNull(future.get());
@@ -3084,7 +3125,8 @@ public class GroupCoordinatorServiceTest {
             (short) 5,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            Duration.ofMillis(100)
         );
 
         assertFutureThrows(CoordinatorNotAvailableException.class, future);
@@ -3104,7 +3146,8 @@ public class GroupCoordinatorServiceTest {
             (short) 5,
             10,
             TransactionResult.COMMIT,
-            TransactionVersion.TV_1.featureLevel()
+            TransactionVersion.TV_1.featureLevel(),
+            Duration.ofMillis(100)
         );
 
         assertFutureThrows(IllegalStateException.class, future);
@@ -3160,11 +3203,13 @@ public class GroupCoordinatorServiceTest {
 
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-topics-deleted"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(offsetFutures);
 
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(shareFutures);
 
@@ -3174,10 +3219,12 @@ public class GroupCoordinatorServiceTest {
         // Wait for the operations to be scheduled and verify method is blocked.
         verify(runtime, timeout(5000).times(1)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-topics-deleted"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         );
         verify(runtime, timeout(5000).times(1)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         );
         assertFalse(resultFuture.isDone());
@@ -3210,10 +3257,12 @@ public class GroupCoordinatorServiceTest {
         // Verify no operations scheduled.
         verify(runtime, times(0)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-topics-deleted"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         );
         verify(runtime, times(0)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         );
     }
@@ -3240,6 +3289,7 @@ public class GroupCoordinatorServiceTest {
         // Mock operations with 3 futures, some failing.
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-topics-deleted"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(Arrays.asList(
             CompletableFuture.completedFuture(null),
@@ -3249,6 +3299,7 @@ public class GroupCoordinatorServiceTest {
 
         when(runtime.scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(Arrays.asList(
             CompletableFuture.completedFuture(null),
@@ -3262,10 +3313,12 @@ public class GroupCoordinatorServiceTest {
         // Verify operations were still scheduled exactly once.
         verify(runtime, times(1)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("on-topics-deleted"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         );
         verify(runtime, times(1)).scheduleWriteAllOperation(
             ArgumentMatchers.eq("maybe-cleanup-share-group-state"),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         );
     }
@@ -3306,6 +3359,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("share-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(
             Map.entry(
@@ -3344,6 +3398,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("share-group-heartbeat"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(exception));
 
@@ -4482,12 +4537,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("complete-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(responseData));
 
@@ -4555,12 +4612,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("complete-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(responseData));
 
@@ -4681,6 +4740,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -4718,6 +4778,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -4753,6 +4814,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_SERVER_ERROR.exception()));
 
@@ -4788,6 +4850,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
@@ -4829,6 +4892,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -4872,6 +4936,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -4913,6 +4978,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -4970,6 +5036,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -5027,6 +5094,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -5087,6 +5155,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -5147,6 +5216,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -5237,6 +5307,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -5308,6 +5379,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -5320,6 +5392,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("complete-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_SERVER_ERROR.exception()));
 
@@ -5432,6 +5505,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initiate-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(deleteShareGroupOffsetsResultHolder));
 
@@ -5444,6 +5518,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("complete-delete-share-group-offsets"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(responseData));
 
@@ -5483,6 +5558,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
@@ -5500,6 +5576,7 @@ public class GroupCoordinatorServiceTest {
         verify(runtime, times(1)).scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(mockPersister, times(1)).initializeState(ArgumentMatchers.any());
@@ -5524,12 +5601,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("uninitialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
@@ -5546,11 +5625,13 @@ public class GroupCoordinatorServiceTest {
         verify(runtime, times(0)).scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(runtime, times(1)).scheduleWriteOperation(
             ArgumentMatchers.eq("uninitialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(mockPersister, times(1)).initializeState(ArgumentMatchers.any());
@@ -5582,12 +5663,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("uninitialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
@@ -5607,11 +5690,13 @@ public class GroupCoordinatorServiceTest {
         verify(runtime, times(0)).scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(runtime, times(1)).scheduleWriteOperation(
             ArgumentMatchers.eq("uninitialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(mockPersister, times(1)).initializeState(ArgumentMatchers.any());
@@ -5648,12 +5733,14 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.failedFuture(exp));
 
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("uninitialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
@@ -5673,11 +5760,13 @@ public class GroupCoordinatorServiceTest {
         verify(runtime, times(1)).scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(runtime, times(1)).scheduleWriteOperation(
             ArgumentMatchers.eq("uninitialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(mockPersister, times(1)).initializeState(ArgumentMatchers.any());
@@ -5772,6 +5861,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("share-group-offsets-alter"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(alterShareGroupOffsetsIntermediate));
 
@@ -5819,6 +5909,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("share-group-offsets-alter"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(FutureUtils.failedFuture(
             new GroupNotEmptyException("bad stuff")
@@ -5868,6 +5959,7 @@ public class GroupCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.eq(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, 0)),
+            ArgumentMatchers.eq(Duration.ofMillis(5000)),
             ArgumentMatchers.any()
         )).thenReturn(CompletableFuture.completedFuture(null));
 
@@ -5875,6 +5967,7 @@ public class GroupCoordinatorServiceTest {
         verify(runtime, times(1)).scheduleWriteOperation(
             ArgumentMatchers.eq("initialize-share-group-state"),
             ArgumentMatchers.any(),
+            ArgumentMatchers.any(),
             ArgumentMatchers.any()
         );
         verify(mockPersister, times(1)).initializeState(ArgumentMatchers.any());
diff --git a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java
index 5ebeb53e3d..922521ec91 100644
--- a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java
+++ b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java
@@ -22153,7 +22153,7 @@ public class GroupMetadataManagerTest {
         expectedMember1 = new ConsumerGroupMember.Builder(memberId1)
             .setState(MemberState.STABLE)
             .setMemberEpoch(1)
-            .setPreviousMemberEpoch(0)
+            .setPreviousMemberEpoch(1)
             .setClientId(DEFAULT_CLIENT_ID)
             .setClientHost(DEFAULT_CLIENT_ADDRESS.toString())
             .setRebalanceTimeoutMs(5000)
@@ -22165,7 +22165,9 @@ public class GroupMetadataManagerTest {
             // The member subscription is updated.
             GroupCoordinatorRecordHelpers.newConsumerGroupMemberSubscriptionRecord(groupId, expectedMember1),
             // The previous regex is deleted.
-            GroupCoordinatorRecordHelpers.newConsumerGroupRegularExpressionTombstone(groupId, "foo*")
+            GroupCoordinatorRecordHelpers.newConsumerGroupRegularExpressionTombstone(groupId, "foo*"),
+            // The previous member epoch is updated.
+            GroupCoordinatorRecordHelpers.newConsumerGroupCurrentAssignmentRecord(groupId, expectedMember1)
         );
 
         assertRecordsEquals(expectedRecords, result.records());
diff --git a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilderTest.java b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilderTest.java
index 751107471e..69ef7330d9 100644
--- a/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilderTest.java
+++ b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/CurrentAssignmentBuilderTest.java
@@ -821,9 +821,10 @@ public class CurrentAssignmentBuilderTest {
         ConsumerGroupMember member = new ConsumerGroupMember.Builder("member")
             .setState(MemberState.STABLE)
             .setMemberEpoch(memberEpoch)
-            .setPreviousMemberEpoch(memberEpoch)
+            .setPreviousMemberEpoch(memberEpoch - 1)
             .setSubscribedTopicNames(List.of(topic2))
             .setAssignedPartitions(mkAssignment(
+                // Topic 1 is assigned, but no longer in the subscription.
                 mkTopicAssignment(topicId1, 1, 2, 3),
                 mkTopicAssignment(topicId2, 4, 5, 6)))
             .build();
@@ -845,6 +846,9 @@ public class CurrentAssignmentBuilderTest {
             new ConsumerGroupMember.Builder("member")
                 .setState(MemberState.STABLE)
                 .setMemberEpoch(expectedMemberEpoch)
+                // The previous member epoch is updated in all cases tested here,
+                // including the case where assigned partitions are removed due to a subscription
+                // change, regardless of whether the member epoch is advanced.
                 .setPreviousMemberEpoch(memberEpoch)
                 .setSubscribedTopicNames(List.of(topic2))
                 .setAssignedPartitions(mkAssignment(
@@ -879,9 +883,10 @@ public class CurrentAssignmentBuilderTest {
         ConsumerGroupMember member = new ConsumerGroupMember.Builder("member")
             .setState(MemberState.STABLE)
             .setMemberEpoch(memberEpoch)
-            .setPreviousMemberEpoch(memberEpoch)
+            .setPreviousMemberEpoch(memberEpoch - 1)
             .setSubscribedTopicNames(List.of(topic2))
             .setAssignedPartitions(mkAssignment(
+                // Topic 1 is assigned, but no longer in the subscription.
                 mkTopicAssignment(topicId1, 1, 2, 3),
                 mkTopicAssignment(topicId2, 4, 5, 6)))
             .build();
@@ -906,6 +911,7 @@ public class CurrentAssignmentBuilderTest {
             new ConsumerGroupMember.Builder("member")
                 .setState(MemberState.UNREVOKED_PARTITIONS)
                 .setMemberEpoch(expectedMemberEpoch)
+                // The previous member epoch is updated in all cases.
                 .setPreviousMemberEpoch(memberEpoch)
                 .setSubscribedTopicNames(List.of(topic2))
                 .setAssignedPartitions(mkAssignment(
diff --git a/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareCoordinatorService.java b/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareCoordinatorService.java
index c8f5c57dc0..8a90cf9742 100644
--- a/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareCoordinatorService.java
+++ b/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareCoordinatorService.java
@@ -201,7 +201,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
                     .withLoader(loader)
                     .withCoordinatorShardBuilderSupplier(supplier)
                     .withTime(time)
-                    .withWriteTimeout(Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()))
+                    .withDefaultWriteTimeOut(Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()))
                     .withCoordinatorRuntimeMetrics(coordinatorRuntimeMetrics)
                     .withCoordinatorMetrics(coordinatorMetrics)
                     .withSerializer(new ShareCoordinatorRecordSerde())
@@ -324,6 +324,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
         runtime.scheduleWriteOperation(
             "write-state-record-prune",
             tp,
+            Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
             ShareCoordinatorShard::lastRedundantOffset
         ).whenComplete((result, exception) -> {
             if (exception != null) {
@@ -383,6 +384,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
                 }
                 List<CompletableFuture<Void>> futures = runtime.scheduleWriteAllOperation(
                     "snapshot-cold-partitions",
+                    Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
                     ShareCoordinatorShard::snapshotColdPartitions
                 );
 
@@ -465,6 +467,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
                     CompletableFuture<WriteShareGroupStateResponseData> future = runtime.scheduleWriteOperation(
                             "write-share-group-state",
                             topicPartitionFor(SharePartitionKey.getInstance(groupId, topicData.topicId(), partitionData.partition())),
+                            Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
                             coordinator -> coordinator.writeState(new WriteShareGroupStateRequestData()
                                 .setGroupId(groupId)
                                 .setTopics(List.of(new WriteShareGroupStateRequestData.WriteStateData()
@@ -598,6 +601,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
                 CompletableFuture<ReadShareGroupStateResponseData> readFuture = runtime.scheduleWriteOperation(
                     "read-update-leader-epoch-state",
                     topicPartitionFor(coordinatorKey),
+                    Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
                     coordinator -> coordinator.readStateAndMaybeUpdateLeaderEpoch(requestForCurrentPartition)
                 ).exceptionally(readException ->
                     handleOperationException(
@@ -707,6 +711,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
                 CompletableFuture<ReadShareGroupStateSummaryResponseData> readFuture = runtime.scheduleWriteOperation(
                     "read-share-group-state-summary",
                     topicPartitionFor(coordinatorKey),
+                    Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
                     coordinator -> coordinator.readStateSummary(requestForCurrentPartition)
                 ).exceptionally(readException ->
                     handleOperationException(
@@ -816,6 +821,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
                 CompletableFuture<DeleteShareGroupStateResponseData> deleteFuture = runtime.scheduleWriteOperation(
                     "delete-share-group-state",
                     topicPartitionFor(coordinatorKey),
+                    Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
                     coordinator -> coordinator.deleteState(requestForCurrentPartition)
                 ).exceptionally(deleteException ->
                     handleOperationException(
@@ -915,6 +921,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
                 CompletableFuture<InitializeShareGroupStateResponseData> initializeFuture = runtime.scheduleWriteOperation(
                     "initialize-share-group-state",
                     topicPartitionFor(coordinatorKey),
+                    Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
                     coordinator -> coordinator.initializeState(requestForCurrentPartition)
                 ).exceptionally(initializeException ->
                     handleOperationException(
@@ -1082,6 +1089,7 @@ public class ShareCoordinatorService implements ShareCoordinator {
             FutureUtils.mapExceptionally(
                 runtime.scheduleWriteAllOperation(
                     "on-topics-deleted",
+                    Duration.ofMillis(config.shareCoordinatorWriteTimeoutMs()),
                     coordinator -> coordinator.maybeCleanupShareState(deletedTopicIds)
                 ),
                 exception -> {
diff --git a/share-coordinator/src/test/java/org/apache/kafka/coordinator/share/ShareCoordinatorServiceTest.java b/share-coordinator/src/test/java/org/apache/kafka/coordinator/share/ShareCoordinatorServiceTest.java
index 4535e8adb7..ea9b8f164f 100644
--- a/share-coordinator/src/test/java/org/apache/kafka/coordinator/share/ShareCoordinatorServiceTest.java
+++ b/share-coordinator/src/test/java/org/apache/kafka/coordinator/share/ShareCoordinatorServiceTest.java
@@ -57,6 +57,7 @@ import org.apache.kafka.server.util.timer.Timer;
 import org.junit.jupiter.api.Test;
 import org.mockito.Mockito;
 
+import java.time.Duration;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
@@ -200,6 +201,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-share-group-state"),
             eq(new TopicPartition(Topic.SHARE_GROUP_STATE_TOPIC_NAME, 0)),
+            eq(Duration.ofMillis(5000)),
             any()
         ))
             .thenReturn(CompletableFuture.completedFuture(response1))
@@ -314,6 +316,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("read-update-leader-epoch-state"),
             eq(new TopicPartition(Topic.SHARE_GROUP_STATE_TOPIC_NAME, 0)),
+            any(),
             any()
         ))
             .thenReturn(CompletableFuture.completedFuture(new ReadShareGroupStateResponseData()
@@ -395,6 +398,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("read-share-group-state-summary"),
             eq(new TopicPartition(Topic.SHARE_GROUP_STATE_TOPIC_NAME, 0)),
+            any(),
             any())
         ).thenReturn(CompletableFuture.completedFuture(new ReadShareGroupStateSummaryResponseData()
             .setResults(List.of(topicData1)))
@@ -475,6 +479,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("delete-share-group-state"),
             eq(new TopicPartition(Topic.SHARE_GROUP_STATE_TOPIC_NAME, 0)),
+            eq(Duration.ofMillis(5000)),
             any()
         )).thenReturn(CompletableFuture.completedFuture(response1)).thenReturn(CompletableFuture.completedFuture(response2));
 
@@ -559,6 +564,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("initialize-share-group-state"),
             eq(new TopicPartition(Topic.SHARE_GROUP_STATE_TOPIC_NAME, 0)),
+            eq(Duration.ofMillis(5000)),
             any())).thenReturn(CompletableFuture.completedFuture(response1)).thenReturn(CompletableFuture.completedFuture(response2)
         );
 
@@ -1175,7 +1181,7 @@ class ShareCoordinatorServiceTest {
         Uuid topicId = Uuid.randomUuid();
         int partition = 0;
 
-        when(runtime.scheduleWriteOperation(any(), any(), any()))
+        when(runtime.scheduleWriteOperation(any(), any(), any(), any()))
             .thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_TOPIC_OR_PARTITION.exception()));
 
         assertEquals(new WriteShareGroupStateResponseData()
@@ -1227,7 +1233,7 @@ class ShareCoordinatorServiceTest {
         Uuid topicId = Uuid.randomUuid();
         int partition = 0;
 
-        when(runtime.scheduleWriteOperation(any(), any(), any()))
+        when(runtime.scheduleWriteOperation(any(), any(), any(), any()))
             .thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_SERVER_ERROR.exception()));
 
         assertEquals(new ReadShareGroupStateResponseData()
@@ -1271,7 +1277,7 @@ class ShareCoordinatorServiceTest {
         Uuid topicId = Uuid.randomUuid();
         int partition = 0;
 
-        when(runtime.scheduleWriteOperation(any(), any(), any()))
+        when(runtime.scheduleWriteOperation(any(), any(), any(), any()))
             .thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_SERVER_ERROR.exception()));
 
         assertEquals(new ReadShareGroupStateSummaryResponseData()
@@ -1315,7 +1321,7 @@ class ShareCoordinatorServiceTest {
         Uuid topicId = Uuid.randomUuid();
         int partition = 0;
 
-        when(runtime.scheduleWriteOperation(any(), any(), any()))
+        when(runtime.scheduleWriteOperation(any(), any(), any(), any()))
             .thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_TOPIC_OR_PARTITION.exception()));
 
         assertEquals(new DeleteShareGroupStateResponseData()
@@ -1358,7 +1364,7 @@ class ShareCoordinatorServiceTest {
         Uuid topicId = Uuid.randomUuid();
         int partition = 0;
 
-        when(runtime.scheduleWriteOperation(any(), any(), any())).thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_TOPIC_OR_PARTITION.exception()));
+        when(runtime.scheduleWriteOperation(any(), any(), any(), any())).thenReturn(FutureUtils.failedFuture(Errors.UNKNOWN_TOPIC_OR_PARTITION.exception()));
 
         assertEquals(
             new InitializeShareGroupStateResponseData().setResults(List.of(new InitializeShareGroupStateResponseData.InitializeStateResult()
@@ -1462,6 +1468,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         )).thenReturn(
             CompletableFuture.completedFuture(Optional.of(10L))
@@ -1488,6 +1495,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // Prune should be called.
@@ -1495,6 +1503,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // Prune should be called.
@@ -1502,6 +1511,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         verify(writer, times(2))
@@ -1542,6 +1552,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             eq(tp1),
+            any(),
             any()
         )).thenReturn(
             CompletableFuture.completedFuture(Optional.of(10L))
@@ -1552,6 +1563,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             eq(tp2),
+            any(),
             any()
         )).thenReturn(
             CompletableFuture.completedFuture(Optional.of(20L))
@@ -1578,6 +1590,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // Prune should be called.
@@ -1585,6 +1598,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // Prune should be called as future completes exceptionally.
@@ -1592,6 +1606,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         verify(writer, times(4))
@@ -1614,6 +1629,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         )).thenReturn(CompletableFuture.failedFuture(Errors.UNKNOWN_SERVER_ERROR.exception()));
 
@@ -1636,6 +1652,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // prune should be called
@@ -1643,6 +1660,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         verify(writer, times(0))
@@ -1664,6 +1682,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         )).thenReturn(CompletableFuture.completedFuture(Optional.of(20L)));
 
@@ -1686,6 +1705,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // prune should be called
@@ -1693,6 +1713,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         verify(writer, times(1))
@@ -1713,6 +1734,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         )).thenReturn(CompletableFuture.completedFuture(Optional.empty()));
 
@@ -1735,6 +1757,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // prune should be called
@@ -1742,6 +1765,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         verify(writer, times(0))
@@ -1770,6 +1794,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         )).thenReturn(
             CompletableFuture.completedFuture(Optional.of(10L))
@@ -1795,6 +1820,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // prune should be called
@@ -1802,6 +1828,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // prune should be called
@@ -1809,6 +1836,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         verify(writer, times(1))
@@ -1841,6 +1869,7 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         )).thenReturn(
             CompletableFuture.completedFuture(Optional.of(10L))
@@ -1867,6 +1896,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // prune should be called
@@ -1874,6 +1904,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(30005L); // prune should be called
@@ -1881,6 +1912,7 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("write-state-record-prune"),
                 any(),
+                any(),
                 any());
 
         verify(writer, times(2))
@@ -1914,6 +1946,7 @@ class ShareCoordinatorServiceTest {
 
         when(runtime.scheduleWriteAllOperation(
             eq("snapshot-cold-partitions"),
+            any(),
             any()
         )).thenReturn(List.of(CompletableFuture.completedFuture(null)));
 
@@ -1923,18 +1956,21 @@ class ShareCoordinatorServiceTest {
             .scheduleWriteOperation(
                 eq("snapshot-cold-partitions"),
                 any(),
+                any(),
                 any());
 
         timer.advanceClock(10005L); // Snapshotting should be called.
         verify(runtime, times(1))
             .scheduleWriteAllOperation(
                 eq("snapshot-cold-partitions"),
+                any(),
                 any());
 
         timer.advanceClock(10005L); // Snapshotting should be called.
         verify(runtime, times(2))
             .scheduleWriteAllOperation(
                 eq("snapshot-cold-partitions"),
+                any(),
                 any());
 
         checkMetrics(metrics);
@@ -1951,6 +1987,7 @@ class ShareCoordinatorServiceTest {
 
         when(runtime.scheduleWriteAllOperation(
             eq("snapshot-cold-partitions"),
+            any(),
             any()
         )).thenReturn(
             List.of(CompletableFuture.completedFuture(null), CompletableFuture.failedFuture(new Exception("bad stuff")))
@@ -1976,18 +2013,21 @@ class ShareCoordinatorServiceTest {
         verify(runtime, times(0))
             .scheduleWriteAllOperation(
                 eq("snapshot-cold-partitions"),
+                any(),
                 any());
 
         timer.advanceClock(10005L); // Snapshotting should be called.
         verify(runtime, times(1))   // For 2 topic partitions.
             .scheduleWriteAllOperation(
                 eq("snapshot-cold-partitions"),
+                any(),
                 any());
 
         timer.advanceClock(10005L); // Snapshotting should be called (despite previous partial failure).
         verify(runtime, times(2))   // For 2 topic partitions.
             .scheduleWriteAllOperation(
                 eq("snapshot-cold-partitions"),
+                any(),
                 any());
 
         checkMetrics(metrics);
@@ -2019,12 +2059,14 @@ class ShareCoordinatorServiceTest {
         when(runtime.scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         )).thenReturn(CompletableFuture.completedFuture(Optional.empty()));
 
         // Snapshot job.
         when(runtime.scheduleWriteAllOperation(
             eq("snapshot-cold-partitions"),
+            any(),
             any()
         )).thenReturn(List.of());
 
@@ -2042,10 +2084,12 @@ class ShareCoordinatorServiceTest {
         verify(runtime, times(0)).scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         );
         verify(runtime, times(0)).scheduleWriteAllOperation(
             eq("snapshot-cold-partitions"),
+            any(),
             any()
         );
         assertFalse(service.shouldRunPeriodicJob());
@@ -2060,10 +2104,12 @@ class ShareCoordinatorServiceTest {
         verify(runtime, times(1)).scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         );
         verify(runtime, times(1)).scheduleWriteAllOperation(
             eq("snapshot-cold-partitions"),
+            any(),
             any()
         );
         assertTrue(service.shouldRunPeriodicJob());
@@ -2078,10 +2124,12 @@ class ShareCoordinatorServiceTest {
         verify(runtime, times(1)).scheduleWriteOperation(
             eq("write-state-record-prune"),
             any(),
+            any(),
             any()
         );
         verify(runtime, times(1)).scheduleWriteAllOperation(
             eq("snapshot-cold-partitions"),
+            any(),
             any()
         );
         assertFalse(service.shouldRunPeriodicJob());
@@ -2148,6 +2196,7 @@ class ShareCoordinatorServiceTest {
 
         when(runtime.scheduleWriteAllOperation(
             eq("on-topics-deleted"),
+            any(),
             any()
         )).thenReturn(
             List.of(
@@ -2160,6 +2209,7 @@ class ShareCoordinatorServiceTest {
         assertDoesNotThrow(() -> service.onTopicsDeleted(Set.of(), BufferSupplier.NO_CACHING));
         verify(runtime, times(0)).scheduleWriteAllOperation(
             eq("on-topics-deleted"),
+            any(),
             any()
         );
     }
@@ -2187,6 +2237,7 @@ class ShareCoordinatorServiceTest {
 
         when(runtime.scheduleWriteAllOperation(
             eq("on-topics-deleted"),
+            any(),
             any()
         )).thenReturn(
             List.of(
@@ -2199,6 +2250,7 @@ class ShareCoordinatorServiceTest {
         assertDoesNotThrow(() -> service.onTopicsDeleted(Set.of(Uuid.randomUuid()), BufferSupplier.NO_CACHING));
         verify(runtime, times(1)).scheduleWriteAllOperation(
             eq("on-topics-deleted"),
+            any(),
             any()
         );
     }
