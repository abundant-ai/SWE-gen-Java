diff --git a/core/src/main/scala/kafka/coordinator/transaction/TransactionLog.scala b/core/src/main/scala/kafka/coordinator/transaction/TransactionLog.scala
new file mode 100644
index 0000000000..f024e88aa8
--- /dev/null
+++ b/core/src/main/scala/kafka/coordinator/transaction/TransactionLog.scala
@@ -0,0 +1,140 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package kafka.coordinator.transaction
+
+import java.nio.ByteBuffer
+import org.apache.kafka.common.compress.Compression
+import org.apache.kafka.common.protocol.{ByteBufferAccessor, MessageUtil}
+import org.apache.kafka.common.record.RecordBatch
+import org.apache.kafka.common.TopicPartition
+import org.apache.kafka.coordinator.transaction.{TransactionMetadata, TransactionState, TxnTransitMetadata}
+import org.apache.kafka.coordinator.transaction.generated.{CoordinatorRecordType, TransactionLogKey, TransactionLogValue}
+import org.apache.kafka.server.common.TransactionVersion
+
+import java.util
+
+import scala.jdk.CollectionConverters._
+
+/**
+ * Messages stored for the transaction topic represent the producer id and transactional status of the corresponding
+ * transactional id, which have versions for both the key and value fields. Key and value
+ * versions are used to evolve the message formats:
+ *
+ * key version 0:               [transactionalId]
+ *    -> value version 0:       [producer_id, producer_epoch, expire_timestamp, status, [topic, [partition] ], timestamp]
+ */
+object TransactionLog {
+
+  // enforce always using
+  //  1. cleanup policy = compact
+  //  2. compression = none
+  //  3. unclean leader election = disabled
+  //  4. required acks = -1 when writing
+  val EnforcedCompression: Compression = Compression.NONE
+  val EnforcedRequiredAcks: Short = (-1).toShort
+
+  /**
+    * Generates the bytes for transaction log message key
+    *
+    * @return key bytes
+    */
+  def keyToBytes(transactionalId: String): Array[Byte] = {
+    MessageUtil.toCoordinatorTypePrefixedBytes(new TransactionLogKey().setTransactionalId(transactionalId))
+  }
+
+  /**
+    * Generates the payload bytes for transaction log message value
+    *
+    * @return value payload bytes
+    */
+  def valueToBytes(txnMetadata: TxnTransitMetadata,
+                                        transactionVersionLevel: TransactionVersion): Array[Byte] = {
+    if (txnMetadata.txnState == TransactionState.EMPTY && !txnMetadata.topicPartitions.isEmpty)
+        throw new IllegalStateException(s"Transaction is not expected to have any partitions since its state is ${txnMetadata.txnState}: $txnMetadata")
+
+      val transactionPartitions = if (txnMetadata.txnState == TransactionState.EMPTY) null
+      else txnMetadata.topicPartitions.asScala
+        .groupBy(_.topic)
+        .map { case (topic, partitions) =>
+          new TransactionLogValue.PartitionsSchema()
+            .setTopic(topic)
+            .setPartitionIds(partitions.map(tp => Integer.valueOf(tp.partition)).toList.asJava)
+        }.toList.asJava
+
+    // Serialize with version 0 (highest non-flexible version) until transaction.version 1 is enabled
+    // which enables flexible fields in records.
+    MessageUtil.toVersionPrefixedBytes(transactionVersionLevel.transactionLogValueVersion(),
+      new TransactionLogValue()
+        .setProducerId(txnMetadata.producerId)
+        .setProducerEpoch(txnMetadata.producerEpoch)
+        .setTransactionTimeoutMs(txnMetadata.txnTimeoutMs)
+        .setTransactionStatus(txnMetadata.txnState.id)
+        .setTransactionLastUpdateTimestampMs(txnMetadata.txnLastUpdateTimestamp)
+        .setTransactionStartTimestampMs(txnMetadata.txnStartTimestamp)
+        .setTransactionPartitions(transactionPartitions)
+        .setClientTransactionVersion(txnMetadata.clientTransactionVersion.featureLevel()))
+  }
+
+  /**
+    * Decodes the transaction log messages' key
+    *
+    * @return left with the version if the key is not a transaction log key, right with the transactional id otherwise
+    */
+  def readTxnRecordKey(buffer: ByteBuffer): Either[Short, String] = {
+    val version = buffer.getShort
+    Either.cond(
+      version == CoordinatorRecordType.TRANSACTION_LOG.id,
+      new TransactionLogKey(new ByteBufferAccessor(buffer), 0.toShort).transactionalId,
+      version
+    )
+  }
+
+  /**
+    * Decodes the transaction log messages' payload and retrieves the transaction metadata from it
+    *
+    * @return a transaction metadata object from the message
+    */
+  def readTxnRecordValue(transactionalId: String, buffer: ByteBuffer): Option[TransactionMetadata] = {
+    // tombstone
+    if (buffer == null) None
+    else {
+      val version = buffer.getShort
+      if (version >= TransactionLogValue.LOWEST_SUPPORTED_VERSION && version <= TransactionLogValue.HIGHEST_SUPPORTED_VERSION) {
+        val value = new TransactionLogValue(new ByteBufferAccessor(buffer), version)
+        val state = TransactionState.fromId(value.transactionStatus)
+        val tps: util.Set[TopicPartition] = new util.HashSet[TopicPartition]()
+        if (!state.equals(TransactionState.EMPTY))
+          value.transactionPartitions.forEach(partitionsSchema => {
+            partitionsSchema.partitionIds.forEach(partitionId => tps.add(new TopicPartition(partitionsSchema.topic, partitionId.intValue())))
+          })
+        Some(new TransactionMetadata(
+          transactionalId,
+          value.producerId,
+          value.previousProducerId,
+          value.nextProducerId,
+          value.producerEpoch,
+          RecordBatch.NO_PRODUCER_EPOCH,
+          value.transactionTimeoutMs,
+          state,
+          tps,
+          value.transactionStartTimestampMs,
+          value.transactionLastUpdateTimestampMs,
+          TransactionVersion.fromFeatureLevel(value.clientTransactionVersion)))
+      } else throw new IllegalStateException(s"Unknown version $version from the transaction log message value")
+    }
+  }
+}
diff --git a/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala b/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala
index 800ac310a1..82b960c5ba 100644
--- a/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala
+++ b/core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala
@@ -35,7 +35,7 @@ import org.apache.kafka.common.requests.ProduceResponse.PartitionResponse
 import org.apache.kafka.common.requests.TransactionResult
 import org.apache.kafka.common.utils.{Time, Utils}
 import org.apache.kafka.common.{KafkaException, TopicIdPartition, TopicPartition}
-import org.apache.kafka.coordinator.transaction.{TransactionLog, TransactionLogConfig, TransactionMetadata, TransactionState, TransactionStateManagerConfig, TxnTransitMetadata}
+import org.apache.kafka.coordinator.transaction.{TransactionLogConfig, TransactionMetadata, TransactionState, TransactionStateManagerConfig, TxnTransitMetadata}
 import org.apache.kafka.metadata.MetadataCache
 import org.apache.kafka.server.common.{RequestLocal, TransactionVersion}
 import org.apache.kafka.server.config.ServerConfigs
@@ -183,7 +183,7 @@ class TransactionStateManager(brokerId: Int,
                 if (recordsBuilder == null) {
                   recordsBuilder = MemoryRecords.builder(
                     ByteBuffer.allocate(math.min(16384, maxBatchSize)),
-                    TransactionLog.ENFORCED_COMPRESSION,
+                    TransactionLog.EnforcedCompression,
                     TimestampType.CREATE_TIME,
                     0L,
                     maxBatchSize
@@ -290,7 +290,7 @@ class TransactionStateManager(brokerId: Int,
     inReadLock(stateLock) {
       replicaManager.appendRecords(
         timeout = config.requestTimeoutMs,
-        requiredAcks = TransactionLog.ENFORCED_REQUIRED_ACKS,
+        requiredAcks = TransactionLog.EnforcedRequiredAcks,
         internalTopicsAllowed = true,
         origin = AppendOrigin.COORDINATOR,
         entriesPerPartition = Map(replicaManager.topicIdPartition(transactionPartition) -> tombstoneRecords),
@@ -495,21 +495,19 @@ class TransactionStateManager(brokerId: Int,
             memRecords.batches.forEach { batch =>
               for (record <- batch.asScala) {
                 require(record.hasKey, "Transaction state log's key should not be null")
-                val transactionalId = try Some(TransactionLog.readTxnRecordKey(record.key))
-                catch {
-                  case e: IllegalStateException =>
-                    warn(s"Unknown message key version while loading transaction state from $topicPartition. " +
-                      s"Ignoring it. It could be a left over from an aborted upgrade", e)
-                    None
-                }
-                transactionalId.foreach { txnId =>
-                  // load transaction metadata along with transaction state
-                  val txnMetadata = TransactionLog.readTxnRecordValue(txnId, record.value)
-                  if (txnMetadata == null) {
-                    loadedTransactions.remove(txnId)
-                  } else {
-                    loadedTransactions.put(txnId, txnMetadata)
-                  }
+                TransactionLog.readTxnRecordKey(record.key) match {
+                  case Left(version) =>
+                    warn(s"Unknown message key with version $version" +
+                      s" while loading transaction state from $topicPartition. Ignoring it. " +
+                      "It could be a left over from an aborted upgrade.")
+                  case Right(transactionalId) =>
+                    // load transaction metadata along with transaction state
+                    TransactionLog.readTxnRecordValue(transactionalId, record.value) match {
+                      case None =>
+                        loadedTransactions.remove(transactionalId)
+                      case Some(txnMetadata) =>
+                        loadedTransactions.put(transactionalId, txnMetadata)
+                    }
                 }
               }
               currOffset = batch.nextOffset
@@ -665,7 +663,7 @@ class TransactionStateManager(brokerId: Int,
     val valueBytes = TransactionLog.valueToBytes(newMetadata, transactionVersionLevel())
     val timestamp = time.milliseconds()
 
-    val records = MemoryRecords.withRecords(TransactionLog.ENFORCED_COMPRESSION, new SimpleRecord(timestamp, keyBytes, valueBytes))
+    val records = MemoryRecords.withRecords(TransactionLog.EnforcedCompression, new SimpleRecord(timestamp, keyBytes, valueBytes))
     val transactionStateTopicPartition = new TopicPartition(Topic.TRANSACTION_STATE_TOPIC_NAME, partitionFor(transactionalId))
     val transactionStateTopicIdPartition = replicaManager.topicIdPartition(transactionStateTopicPartition)
     val recordsPerPartition = Map(transactionStateTopicIdPartition -> records)
@@ -808,7 +806,7 @@ class TransactionStateManager(brokerId: Int,
           if (append) {
             replicaManager.appendRecords(
               timeout = newMetadata.txnTimeoutMs.toLong,
-              requiredAcks = TransactionLog.ENFORCED_REQUIRED_ACKS,
+              requiredAcks = TransactionLog.EnforcedRequiredAcks,
               internalTopicsAllowed = true,
               origin = AppendOrigin.COORDINATOR,
               entriesPerPartition = recordsPerPartition,
diff --git a/server-common/src/main/java/org/apache/kafka/server/share/persister/PartitionFactory.java b/server-common/src/main/java/org/apache/kafka/server/share/persister/PartitionFactory.java
index 215e95ff08..b36c483a53 100644
--- a/server-common/src/main/java/org/apache/kafka/server/share/persister/PartitionFactory.java
+++ b/server-common/src/main/java/org/apache/kafka/server/share/persister/PartitionFactory.java
@@ -42,7 +42,12 @@ public class PartitionFactory {
     }
 
     public static PartitionStateData newPartitionStateData(int partition, int stateEpoch, long startOffset) {
-        return new PartitionData(partition, stateEpoch, startOffset, UNINITIALIZED_DELIVERY_COMPLETE_COUNT, DEFAULT_ERROR_CODE, DEFAULT_ERR_MESSAGE, DEFAULT_LEADER_EPOCH, null);
+        // If the start offset is uninitialized (when the share partition is being initialized for the first time), the 
+        // consumption hasn't started yet, and lag cannot be calculated. Thus, deliveryCompleteCount is also set as -1. 
+        // But, if start offset is a non-negative value (when the start offset is altered), the lag can be calculated 
+        // from that point onward. Hence, we set deliveryCompleteCount to 0 in that case.
+        int deliveryCompleteCount = startOffset == UNINITIALIZED_START_OFFSET ? UNINITIALIZED_DELIVERY_COMPLETE_COUNT : 0;
+        return new PartitionData(partition, stateEpoch, startOffset, deliveryCompleteCount, DEFAULT_ERROR_CODE, DEFAULT_ERR_MESSAGE, DEFAULT_LEADER_EPOCH, null);
     }
 
     public static PartitionErrorData newPartitionErrorData(int partition, short errorCode, String errorMessage) {
diff --git a/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareGroupOffset.java b/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareGroupOffset.java
index 9f1a264425..ac17045733 100644
--- a/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareGroupOffset.java
+++ b/share-coordinator/src/main/java/org/apache/kafka/coordinator/share/ShareGroupOffset.java
@@ -35,6 +35,7 @@ public class ShareGroupOffset {
     public static final int NO_TIMESTAMP = 0;
     public static final int UNINITIALIZED_EPOCH = 0;
     public static final int UNINITIALIZED_DELIVERY_COMPLETE_COUNT = -1;
+    public static final int UNINITIALIZED_START_OFFSET = -1;
     public static final int DEFAULT_EPOCH = 0;
 
     private final int snapshotEpoch;
@@ -160,14 +161,18 @@ public class ShareGroupOffset {
     }
 
     public static ShareGroupOffset fromRequest(InitializeShareGroupStateRequestData.PartitionData data, int snapshotEpoch, long timestamp) {
-        // This method is invoked during InitializeShareGroupStateRequest. Since the deliveryCompleteCount is not yet
-        // known at this stage, it is initialized to its default value.
+        // This method is invoked during InitializeShareGroupStateRequest. If the start offset is uninitialized (when the 
+        // share partition is being initialized for the first time), the consumption hasn't started yet, and lag cannot
+        // be calculated. Thus, deliveryCompleteCount is also set as -1. But, if start offset is a non-negative value (when 
+        // the start offset is altered), the lag can be calculated from that point onward. Hence, we set deliveryCompleteCount
+        // to 0 in that case.
+        int deliveryCompleteCount = data.startOffset() == UNINITIALIZED_START_OFFSET ? UNINITIALIZED_DELIVERY_COMPLETE_COUNT : 0;
         return new ShareGroupOffset(
             snapshotEpoch,
             data.stateEpoch(),
             UNINITIALIZED_EPOCH,
             data.startOffset(),
-            UNINITIALIZED_DELIVERY_COMPLETE_COUNT,
+            deliveryCompleteCount,
             List.of(),
             timestamp,
             timestamp
diff --git a/transaction-coordinator/src/main/java/org/apache/kafka/coordinator/transaction/TransactionLog.java b/transaction-coordinator/src/main/java/org/apache/kafka/coordinator/transaction/TransactionLog.java
deleted file mode 100644
index 9c86953b5e..0000000000
--- a/transaction-coordinator/src/main/java/org/apache/kafka/coordinator/transaction/TransactionLog.java
+++ /dev/null
@@ -1,160 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.kafka.coordinator.transaction;
-
-import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.compress.Compression;
-import org.apache.kafka.common.protocol.ByteBufferAccessor;
-import org.apache.kafka.common.protocol.MessageUtil;
-import org.apache.kafka.common.record.RecordBatch;
-import org.apache.kafka.coordinator.transaction.generated.CoordinatorRecordType;
-import org.apache.kafka.coordinator.transaction.generated.TransactionLogKey;
-import org.apache.kafka.coordinator.transaction.generated.TransactionLogValue;
-import org.apache.kafka.server.common.TransactionVersion;
-
-import java.nio.ByteBuffer;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-import java.util.stream.Collectors;
-
-/**
- * Messages stored for the transaction topic represent the producer id and transactional status of the corresponding
- * transactional id, which have versions for both the key and value fields. Key and value
- * versions are used to evolve the message formats:
- *
- * key version 0:               [transactionalId]
- *    -> value version 0:       [producer_id, producer_epoch, expire_timestamp, status, [topic, [partition] ], timestamp]
- */
-public class TransactionLog {
-
-    // enforce always using
-    //  1. cleanup policy = compact
-    //  2. compression = none
-    //  3. unclean leader election = disabled
-    //  4. required acks = -1 when writing
-    public static final Compression ENFORCED_COMPRESSION = Compression.NONE;
-    public static final short ENFORCED_REQUIRED_ACKS = (short) -1;
-
-    /**
-     * Generates the bytes for transaction log message key
-     *
-     * @return key bytes
-     */
-    public static byte[] keyToBytes(String transactionalId) {
-        return MessageUtil.toCoordinatorTypePrefixedBytes(
-                new TransactionLogKey().setTransactionalId(transactionalId)
-        );
-    }
-
-    /**
-     * Generates the payload bytes for transaction log message value
-     *
-     * @return value payload bytes
-     */
-    public static byte[] valueToBytes(TxnTransitMetadata txnMetadata,
-                                      TransactionVersion transactionVersionLevel) {
-        if (txnMetadata.txnState() == TransactionState.EMPTY && !txnMetadata.topicPartitions().isEmpty()) {
-            throw new IllegalStateException("Transaction is not expected to have any partitions since its state is "
-                    + txnMetadata.txnState() + ": " + txnMetadata);
-        }
-
-        List<TransactionLogValue.PartitionsSchema> transactionPartitions = null;
-
-        if (txnMetadata.txnState() != TransactionState.EMPTY) {
-            transactionPartitions = txnMetadata.topicPartitions().stream()
-                    .collect(Collectors.groupingBy(TopicPartition::topic))
-                    .entrySet().stream()
-                    .map(entry ->
-                        new TransactionLogValue.PartitionsSchema().setTopic(entry.getKey())
-                            .setPartitionIds(entry.getValue().stream().map(TopicPartition::partition).toList())).toList();
-        }
-
-        return MessageUtil.toVersionPrefixedBytes(
-                transactionVersionLevel.transactionLogValueVersion(),
-                new TransactionLogValue()
-                        .setProducerId(txnMetadata.producerId())
-                        .setProducerEpoch(txnMetadata.producerEpoch())
-                        .setTransactionTimeoutMs(txnMetadata.txnTimeoutMs())
-                        .setTransactionStatus(txnMetadata.txnState().id())
-                        .setTransactionLastUpdateTimestampMs(txnMetadata.txnLastUpdateTimestamp())
-                        .setTransactionStartTimestampMs(txnMetadata.txnStartTimestamp())
-                        .setTransactionPartitions(transactionPartitions)
-                        .setClientTransactionVersion(txnMetadata.clientTransactionVersion().featureLevel())
-        );
-    }
-
-    /**
-     * Decodes the transaction log messages' key
-     *
-     * @return the transactional id
-     * @throws IllegalStateException if the version is not a valid transaction log key version
-     */
-    public static String readTxnRecordKey(ByteBuffer buffer) {
-        short version = buffer.getShort();
-        if (version == CoordinatorRecordType.TRANSACTION_LOG.id()) {
-            return new TransactionLogKey(new ByteBufferAccessor(buffer), (short) 0).transactionalId();
-        } else {
-            throw new IllegalStateException("Unknown version " + version + " from the transaction log message key");
-        }
-    }
-
-    /**
-     * Decodes the transaction log messages' payload and retrieves the transaction metadata from it
-     *
-     * @return a transaction metadata object from the message, or null if tombstone
-     */
-    public static TransactionMetadata readTxnRecordValue(String transactionalId, ByteBuffer buffer) {
-        if (buffer == null) {
-            return null; // tombstone
-        } else {
-            short version = buffer.getShort();
-            if (version >= TransactionLogValue.LOWEST_SUPPORTED_VERSION
-                    && version <= TransactionLogValue.HIGHEST_SUPPORTED_VERSION) {
-
-                TransactionLogValue value = new TransactionLogValue(new ByteBufferAccessor(buffer), version);
-                TransactionState state = TransactionState.fromId(value.transactionStatus());
-
-                Set<TopicPartition> tps = new HashSet<>();
-                if (state != TransactionState.EMPTY) {
-                    for (TransactionLogValue.PartitionsSchema partitionsSchema : value.transactionPartitions()) {
-                        for (int partitionId : partitionsSchema.partitionIds()) {
-                            tps.add(new TopicPartition(partitionsSchema.topic(), partitionId));
-                        }
-                    }
-                }
-
-                return new TransactionMetadata(
-                        transactionalId,
-                        value.producerId(),
-                        value.previousProducerId(),
-                        value.nextProducerId(),
-                        value.producerEpoch(),
-                        RecordBatch.NO_PRODUCER_EPOCH,
-                        value.transactionTimeoutMs(),
-                        state,
-                        tps,
-                        value.transactionStartTimestampMs(),
-                        value.transactionLastUpdateTimestampMs(),
-                        TransactionVersion.fromFeatureLevel(value.clientTransactionVersion())
-                );
-            } else {
-                throw new IllegalStateException("Unknown version " + version + " from the transaction log message value");
-            }
-        }
-    }
-}
